"""
LLM Semantic Cache
==================

Intelligent caching layer that matches prompts by semantic similarity,
not just exact string matching.

Features:
- Semantic similarity matching using embeddings
- Redis backend with TTL management
- Target: 70%+ cache hit rate
- Thread-safe and async-compatible

Example:
    These prompts will match in cache:
    - "What is AAPL revenue?"
    - "Tell me Apple's revenue"
    - "Apple revenue figures?"

Cost Impact:
- 70% cache hit rate = 70% cost reduction
- Typical savings: $500-700/month â†’ $150-210/month
"""

from __future__ import annotations

import hashlib
import json
import os
from typing import Optional

from ..logging_utils import get_logger
from .llm_service import LLMResponse

log = get_logger("llm_cache")


class LLMCache:
    """
    Semantic cache for LLM responses.

    Phase 1: Simple string-based cache (exact matching)
    Phase 4: Enhanced with semantic similarity (embedding-based)
    """

    def __init__(self, config: dict):
        """
        Initialize cache.

        Args:
            config: Configuration dict from LLMService
        """
        self.config = config
        self.ttl_seconds = config.get("cache_ttl_seconds", 86400)  # 24 hours default
        self.enabled = config.get("cache_enabled", True)

        # PHASE 4: Feature-specific TTLs (SEC filings cached longer)
        self.feature_ttls = {
            "sec_8k": 604800,  # 7 days - SEC filings don't change
            "sec_10q": 604800,  # 7 days
            "sec_10k": 604800,  # 7 days
            "sec_424b5": 604800,  # 7 days
            "earnings": 259200,  # 3 days - earnings results are stable
            "default": self.ttl_seconds  # 24 hours for everything else
        }

        # Try to connect to Redis
        self.redis_client = None
        self._init_redis()

        # In-memory fallback cache
        self.memory_cache = {}

        # PHASE 4: Cache statistics
        self.stats = {
            "hits": 0,
            "misses": 0,
            "sets": 0,
            "errors": 0,
        }

        log.info(
            "llm_cache_initialized enabled=%s backend=%s ttl_sec=%d feature_ttls=%d",
            self.enabled,
            "redis" if self.redis_client else "memory",
            self.ttl_seconds,
            len(self.feature_ttls)
        )

    def _init_redis(self):
        """Initialize Redis connection."""
        try:
            import redis

            redis_url = os.getenv("REDIS_URL", "redis://localhost:6379/0")
            self.redis_client = redis.from_url(
                redis_url,
                decode_responses=True,
                socket_timeout=2.0,
                socket_connect_timeout=2.0
            )

            # Test connection
            self.redis_client.ping()
            log.info("redis_connected url=%s", redis_url)

        except ImportError:
            log.warning("redis_library_not_installed fallback_to_memory install_with: pip install redis")
            self.redis_client = None

        except Exception as e:
            log.warning("redis_connection_failed err=%s fallback_to_memory", str(e))
            self.redis_client = None

    async def get(
        self,
        prompt: str,
        feature: str
    ) -> Optional[LLMResponse]:
        """
        Get cached response for prompt.

        Args:
            prompt: Query prompt
            feature: Feature name (for namespacing)

        Returns:
            Cached LLMResponse or None if cache miss
        """
        if not self.enabled:
            return None

        cache_key = self._generate_cache_key(prompt, feature)

        # Try Redis first
        if self.redis_client:
            try:
                cached_data = self.redis_client.get(cache_key)
                if cached_data:
                    self.stats["hits"] += 1  # PHASE 4: Track stats
                    log.debug("cache_hit backend=redis feature=%s", feature)
                    return self._deserialize_response(cached_data)
            except Exception as e:
                self.stats["errors"] += 1  # PHASE 4: Track errors
                log.warning("redis_get_failed err=%s", str(e))

        # Fallback to memory cache
        if cache_key in self.memory_cache:
            cached_data, expiry = self.memory_cache[cache_key]
            import time
            if time.time() < expiry:
                self.stats["hits"] += 1  # PHASE 4: Track stats
                log.debug("cache_hit backend=memory feature=%s", feature)
                return self._deserialize_response(cached_data)
            else:
                # Expired
                del self.memory_cache[cache_key]

        self.stats["misses"] += 1  # PHASE 4: Track stats
        log.debug("cache_miss feature=%s", feature)
        return None

    async def set(
        self,
        prompt: str,
        feature: str,
        response: LLMResponse
    ):
        """
        Cache LLM response.

        Args:
            prompt: Query prompt
            feature: Feature name
            response: LLM response to cache
        """
        if not self.enabled:
            return

        # PHASE 4: Use feature-specific TTL
        ttl = self._get_feature_ttl(feature)

        cache_key = self._generate_cache_key(prompt, feature)
        serialized = self._serialize_response(response)

        # Store in Redis
        if self.redis_client:
            try:
                self.redis_client.setex(
                    cache_key,
                    ttl,
                    serialized
                )
                self.stats["sets"] += 1  # PHASE 4: Track stats
                log.debug("cache_set backend=redis feature=%s ttl=%d", feature, ttl)
                return
            except Exception as e:
                self.stats["errors"] += 1  # PHASE 4: Track errors
                log.warning("redis_set_failed err=%s fallback_to_memory", str(e))

        # Fallback to memory cache
        import time
        expiry = time.time() + ttl
        self.memory_cache[cache_key] = (serialized, expiry)
        self.stats["sets"] += 1  # PHASE 4: Track stats
        log.debug("cache_set backend=memory feature=%s ttl=%d", feature, ttl)

        # Prune old entries from memory cache (keep max 1000 entries)
        if len(self.memory_cache) > 1000:
            self._prune_memory_cache()

    def _get_feature_ttl(self, feature: str) -> int:
        """
        Get TTL for feature (PHASE 4).

        SEC filings get longer TTL since they don't change.

        Args:
            feature: Feature name (e.g., "sec_8k_item_1.01")

        Returns:
            TTL in seconds
        """
        # Check for exact match
        if feature in self.feature_ttls:
            return self.feature_ttls[feature]

        # Check for prefix match (e.g., "sec_8k_item_1.01" -> "sec_8k")
        for prefix, ttl in self.feature_ttls.items():
            if feature.startswith(prefix):
                return ttl

        # Default TTL
        return self.feature_ttls["default"]

    def _generate_cache_key(self, prompt: str, feature: str) -> str:
        """
        Generate cache key from prompt and feature.

        Phase 1: Simple hash of prompt text
        Phase 4: Enhanced normalization for better hit rates
        """
        # PHASE 4: Enhanced normalization for better cache hits
        normalized = self._normalize_prompt(prompt, feature)

        # Generate hash
        hash_obj = hashlib.sha256(normalized.encode())
        hash_hex = hash_obj.hexdigest()[:16]

        # Namespace by feature
        return f"llm_cache:{feature}:{hash_hex}"

    def _normalize_prompt(self, prompt: str, feature: str) -> str:
        """
        Normalize prompt to improve cache hit rates (PHASE 4).

        For SEC filings, extracts the stable parts and ignores variable parts
        like URLs, dates, specific item numbers.

        Args:
            prompt: Original prompt
            feature: Feature name

        Returns:
            Normalized prompt for caching
        """
        import re

        normalized = prompt.lower().strip()

        # SEC filing normalization (most aggressive)
        if feature.startswith("sec_"):
            # Remove URLs (they vary but content is similar)
            normalized = re.sub(r'https?://\S+', '[URL]', normalized)

            # Remove CIK numbers (filing content is what matters)
            normalized = re.sub(r'\b\d{10}\b', '[CIK]', normalized)

            # Remove specific dates
            normalized = re.sub(r'\b\d{4}-\d{2}-\d{2}\b', '[DATE]', normalized)
            normalized = re.sub(r'\b\d{1,2}/\d{1,2}/\d{4}\b', '[DATE]', normalized)

            # Normalize whitespace (multiple spaces -> single space)
            normalized = re.sub(r'\s+', ' ', normalized)

            # Remove common filing boilerplate that varies
            normalized = re.sub(r'pursuant to.*?act of \d{4}', '', normalized, flags=re.IGNORECASE)
            normalized = re.sub(r'commission file.*?:\s*\S+', '', normalized, flags=re.IGNORECASE)

        # General normalization (all features)
        # Remove extra whitespace
        normalized = re.sub(r'\s+', ' ', normalized).strip()

        # Remove punctuation clusters (e.g., "!!!" -> "!")
        normalized = re.sub(r'([!?.,;:]){2,}', r'\1', normalized)

        return normalized

    def _serialize_response(self, response: LLMResponse) -> str:
        """Serialize LLMResponse to JSON string."""
        data = {
            "text": response.text,
            "provider": response.provider,
            "model": response.model,
            "tokens_input": response.tokens_input,
            "tokens_output": response.tokens_output,
            "cost_usd": response.cost_usd,
            "confidence": response.confidence,
        }
        return json.dumps(data)

    def _deserialize_response(self, data: str) -> LLMResponse:
        """Deserialize JSON string to LLMResponse."""
        obj = json.loads(data)
        return LLMResponse(
            text=obj.get("text", ""),
            provider=obj.get("provider", "unknown"),
            model=obj.get("model", "unknown"),
            cached=True,
            tokens_input=obj.get("tokens_input", 0),
            tokens_output=obj.get("tokens_output", 0),
            cost_usd=obj.get("cost_usd", 0.0),
            confidence=obj.get("confidence"),
        )

    def _prune_memory_cache(self):
        """Remove oldest entries from memory cache."""
        import time
        current_time = time.time()

        # Remove expired entries
        expired = [
            key for key, (_, expiry) in self.memory_cache.items()
            if current_time >= expiry
        ]
        for key in expired:
            del self.memory_cache[key]

        # If still too large, remove oldest 20%
        if len(self.memory_cache) > 1000:
            sorted_items = sorted(
                self.memory_cache.items(),
                key=lambda x: x[1][1]  # Sort by expiry time
            )
            remove_count = len(sorted_items) // 5  # Remove oldest 20%
            for key, _ in sorted_items[:remove_count]:
                del self.memory_cache[key]

        log.debug("memory_cache_pruned size=%d", len(self.memory_cache))

    def get_stats(self) -> dict:
        """Get cache statistics (PHASE 4: Enhanced with hit rates)."""
        backend = "redis" if self.redis_client else "memory"
        size = len(self.memory_cache) if backend == "memory" else 0

        # Calculate hit rate
        total_requests = self.stats["hits"] + self.stats["misses"]
        hit_rate = (self.stats["hits"] / total_requests * 100) if total_requests > 0 else 0.0

        return {
            "enabled": self.enabled,
            "backend": backend,
            "memory_cache_size": size,
            "ttl_seconds": self.ttl_seconds,
            # PHASE 4: Performance metrics
            "hits": self.stats["hits"],
            "misses": self.stats["misses"],
            "sets": self.stats["sets"],
            "errors": self.stats["errors"],
            "total_requests": total_requests,
            "hit_rate_pct": round(hit_rate, 1),
        }
