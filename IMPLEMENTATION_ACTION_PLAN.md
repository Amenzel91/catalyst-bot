# Implementation Action Plan: Cloud-First LLM + WebSocket SEC Digester

**Version**: 1.0
**Date**: 2025-11-17
**Status**: Ready for Implementation
**Timeline**: 7 weeks
**Development Tools**: Claude Code, Codex CLI

---

## Table of Contents

1. [Overview](#overview)
2. [Prerequisites](#prerequisites)
3. [Project Structure](#project-structure)
4. [Phase 1: Foundation - LLM Service Hub](#phase-1-foundation---llm-service-hub)
5. [Phase 2: WebSocket SEC Digester Service](#phase-2-websocket-sec-digester-service)
6. [Phase 3: Client Integration](#phase-3-client-integration)
7. [Phase 4: Migration & Optimization](#phase-4-migration--optimization)
8. [Phase 5: Production Deployment](#phase-5-production-deployment)
9. [Testing Strategy](#testing-strategy)
10. [Rollback Plan](#rollback-plan)
11. [Appendix: Scripts & Templates](#appendix-scripts--templates)

---

## Overview

This plan implements a **cloud-first, microservices architecture** combining:
- **Unified LLM Service Hub** - Centralized API-only LLM routing (Gemini + Claude)
- **WebSocket SEC Digester** - Standalone service for real-time filing analysis
- **Lightweight Main Bot** - WebSocket client for Discord alerts

### Key Decisions Made

‚úÖ **No Local LLM** - API-only (fixes GPU overload)
‚úÖ **WebSocket Architecture** - SEC Digester as standalone service
‚úÖ **Redis Streams** - Message persistence and reliability
‚úÖ **Cloud Deployment** - SEC service in cloud, bot runs anywhere

### Success Criteria

- [ ] SEC filings analyzed in <2s average
- [ ] 70%+ cache hit rate on LLM queries
- [ ] <50ms WebSocket latency
- [ ] Zero message loss (Redis Streams)
- [ ] Monthly cost < $1,000
- [ ] 50% code reduction (5,100 ‚Üí 2,500 lines)

---

## Prerequisites

### Required Before Starting

#### 1. API Keys
```bash
# Google Cloud - Gemini API
# Get key from: https://makersuite.google.com/app/apikey
export GEMINI_API_KEY="your_key_here"

# Anthropic - Claude API
# Get key from: https://console.anthropic.com/
export ANTHROPIC_API_KEY="your_key_here"

# Test connectivity
curl -H "Content-Type: application/json" \
  -H "x-goog-api-key: $GEMINI_API_KEY" \
  https://generativelanguage.googleapis.com/v1beta/models
```

#### 2. Infrastructure
```bash
# Redis (for caching + streams)
docker run -d --name redis -p 6379:6379 redis:7-alpine

# Or managed Redis (recommended for production)
# AWS ElastiCache, Redis Cloud, etc.

# Verify Redis
redis-cli ping  # Should return "PONG"
```

#### 3. Development Environment
```bash
# Python 3.11+
python --version  # Should be 3.11 or higher

# Install poetry for dependency management
curl -sSL https://install.python-poetry.org | python3 -

# Verify
poetry --version
```

#### 4. Git Branch
```bash
# Create feature branch
git checkout -b feature/cloud-llm-websocket-integration

# Set up tracking
git push -u origin feature/cloud-llm-websocket-integration
```

#### 5. Disable Local Mistral
```bash
# Stop Ollama service
sudo systemctl stop ollama
sudo systemctl disable ollama

# Verify GPU is idle
watch -n 1 rocm-smi  # Should show low usage
```

### Verification Checklist

- [ ] Gemini API key working (1,500 free requests/day)
- [ ] Claude API key working
- [ ] Redis running locally or cloud instance ready
- [ ] Python 3.11+ installed
- [ ] Poetry installed
- [ ] Git branch created
- [ ] Ollama disabled (GPU at idle)

---

## Project Structure

### New Directory Layout

```
catalyst-bot/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ catalyst_bot/              # Existing bot code
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/              # NEW: Centralized services
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm/               # LLM service hub
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ service.py     # Main service interface
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ router.py      # Intelligent routing
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cache.py       # Semantic caching
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompts.py     # Template management
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ monitor.py     # Usage tracking
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ providers/     # API adapters
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ base.py    # Abstract base
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ gemini.py  # Google Gemini
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ claude.py  # Anthropic Claude
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ websocket/         # WebSocket client
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ client.py      # SEC WebSocket client
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ handlers.py    # Message handlers
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [existing files...]
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ sec_digester_service/      # NEW: Standalone SEC service
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ main.py                # FastAPI app
‚îÇ       ‚îú‚îÄ‚îÄ config.py              # Service configuration
‚îÇ       ‚îú‚îÄ‚îÄ models.py              # Data models
‚îÇ       ‚îú‚îÄ‚îÄ edgar/                 # EDGAR integration
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ monitor.py         # Filing monitor
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ fetcher.py         # Document fetcher
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ parser.py          # Filing parser
‚îÇ       ‚îú‚îÄ‚îÄ analysis/              # Analysis logic
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ analyzer.py        # Main analyzer
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ numeric.py         # Numeric extraction
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ xbrl.py            # XBRL parsing
‚îÇ       ‚îú‚îÄ‚îÄ websocket/             # WebSocket server
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ server.py          # WebSocket endpoints
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ broadcaster.py     # Redis ‚Üí WebSocket
‚îÇ       ‚îî‚îÄ‚îÄ streams/               # Redis Streams
‚îÇ           ‚îú‚îÄ‚îÄ __init__.py
‚îÇ           ‚îú‚îÄ‚îÄ publisher.py       # Publish to Redis
‚îÇ           ‚îî‚îÄ‚îÄ consumer.py        # Consume from Redis
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ services/                  # NEW: Service tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_service.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_router.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_cache.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_providers.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ websocket/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ test_client.py
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ sec_digester_service/      # NEW: SEC service tests
‚îÇ       ‚îú‚îÄ‚îÄ test_main.py
‚îÇ       ‚îú‚îÄ‚îÄ test_analyzer.py
‚îÇ       ‚îî‚îÄ‚îÄ test_websocket.py
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ architecture/              # NEW: Architecture docs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LLM_SERVICE.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SEC_WEBSOCKET.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ DEPLOYMENT.md
‚îÇ   ‚îî‚îÄ‚îÄ guides/                    # NEW: Implementation guides
‚îÇ       ‚îú‚îÄ‚îÄ LLM_INTEGRATION.md
‚îÇ       ‚îú‚îÄ‚îÄ WEBSOCKET_CLIENT.md
‚îÇ       ‚îî‚îÄ‚îÄ TESTING.md
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ setup_dev_env.sh           # NEW: Dev environment setup
‚îÇ   ‚îú‚îÄ‚îÄ test_llm_providers.py      # NEW: Test API connectivity
‚îÇ   ‚îú‚îÄ‚îÄ test_websocket.py          # NEW: Test WebSocket
‚îÇ   ‚îî‚îÄ‚îÄ migrate_sec_code.py        # NEW: Migration helper
‚îÇ
‚îú‚îÄ‚îÄ deployments/
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml         # NEW: Local development
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.prod.yml    # NEW: Production
‚îÇ   ‚îî‚îÄ‚îÄ k8s/                       # NEW: Kubernetes configs (optional)
‚îÇ       ‚îú‚îÄ‚îÄ sec-digester.yaml
‚îÇ       ‚îî‚îÄ‚îÄ redis.yaml
‚îÇ
‚îú‚îÄ‚îÄ .env.example                   # NEW: Example environment vars
‚îú‚îÄ‚îÄ pyproject.toml                 # Updated dependencies
‚îî‚îÄ‚îÄ README.md                      # Updated docs

```

### Key Changes from Current Structure

**Additions:**
- `services/llm/` - Unified LLM hub (replaces scattered LLM files)
- `services/websocket/` - WebSocket client for main bot
- `sec_digester_service/` - Standalone SEC service
- `deployments/` - Docker and K8s configs
- `scripts/` - Helper scripts for setup and testing

**Removals (to be deprecated):**
- `llm_client.py` ‚Üí Replaced by `services/llm/`
- `llm_async.py` ‚Üí Replaced by `services/llm/`
- `llm_hybrid.py` ‚Üí Replaced by `services/llm/router.py`
- `llm_chain.py` ‚Üí Replaced by `services/llm/service.py`
- `sec_llm_analyzer.py` ‚Üí Moved to `sec_digester_service/analysis/`
- `sec_llm_cache.py` ‚Üí Merged into `services/llm/cache.py`

---

## Phase 1: Foundation - LLM Service Hub

**Duration**: 2 weeks
**Goal**: Build centralized, cloud-only LLM service

### Context for Claude Code

When implementing this phase, you're building a **unified abstraction layer** over multiple LLM providers. The key pattern is:

1. **Single Interface** (`LLMService`) - All features call this
2. **Intelligent Router** - Selects best model for task complexity
3. **Semantic Cache** - Avoids duplicate API calls (70% hit rate target)
4. **Provider Adapters** - Swap providers without changing callers

**Design Philosophy**:
- Async-first (use `asyncio` everywhere)
- Type-safe (use `Pydantic` for all data)
- Fail-fast (validate early, clear errors)
- Observable (log everything, track costs)

---

### Ticket 1.1: Project Setup & Dependencies

**Priority**: Critical
**Estimated Time**: 2 hours
**Dependencies**: None

#### Context

Setting up the foundation for cloud-first development. We're removing all local LLM dependencies (Ollama) and adding cloud API clients.

#### Implementation Steps

1. **Update `pyproject.toml`**

```toml
# pyproject.toml
[tool.poetry]
name = "catalyst-bot"
version = "2.0.0"
description = "Trading bot with cloud-first LLM and WebSocket SEC digester"
python = "^3.11"

[tool.poetry.dependencies]
python = "^3.11"

# Web frameworks
fastapi = "^0.109.0"
uvicorn = {extras = ["standard"], version = "^0.27.0"}
websockets = "^12.0"
aiohttp = "^3.9.0"

# LLM providers (NO Ollama)
google-generativeai = "^0.3.0"  # Gemini
anthropic = "^0.18.0"            # Claude

# Data & validation
pydantic = "^2.5.0"
pydantic-settings = "^2.1.0"

# Caching & messaging
redis = {extras = ["hiredis"], version = "^5.0.0"}
sentence-transformers = "^2.3.0"  # Semantic cache

# Monitoring
prometheus-client = "^0.19.0"

# Utilities
python-dotenv = "^1.0.0"
structlog = "^24.1.0"

# Existing dependencies...
discord-py = "^2.3.2"
# ... (keep all existing)

[tool.poetry.group.dev.dependencies]
pytest = "^7.4.0"
pytest-asyncio = "^0.23.0"
pytest-cov = "^4.1.0"
black = "^23.12.0"
ruff = "^0.1.0"
mypy = "^1.8.0"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
```

2. **Install dependencies**

```bash
# Remove old dependencies (Ollama-related)
poetry remove ollama llama-cpp-python  # If present

# Install new dependencies
poetry install

# Update lock file
poetry lock --no-update
```

3. **Create `.env.example`**

```bash
# .env.example

# ============================================================================
# LLM Provider Configuration
# ============================================================================

# Google Gemini API
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_MODEL_DEFAULT=gemini-2.5-flash

# Anthropic Claude API
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_MODEL_DEFAULT=claude-3-5-sonnet-20241022

# ============================================================================
# LLM Service Configuration
# ============================================================================

# Routing strategy: cost_optimized, performance, balanced
LLM_ROUTING_STRATEGY=cost_optimized

# Complexity threshold for Pro tier (0.0 - 1.0)
LLM_COMPLEXITY_THRESHOLD=0.7

# Enable cost tracking
LLM_COST_TRACKING=true
LLM_COST_ALERT_DAILY=30.00
LLM_COST_ALERT_MONTHLY=800.00
LLM_COST_HARD_LIMIT_MONTHLY=1000.00

# ============================================================================
# Caching Configuration
# ============================================================================

# Redis connection
REDIS_URL=redis://localhost:6379

# Cache settings
LLM_CACHE_ENABLED=true
LLM_CACHE_TTL_SECONDS=86400
LLM_CACHE_SIMILARITY_THRESHOLD=0.95

# ============================================================================
# SEC Digester Service
# ============================================================================

# WebSocket connection (will be set in Phase 2)
# SEC_DIGESTER_URL=ws://localhost:8765/ws/filings

# ============================================================================
# Existing Configuration
# ============================================================================

# Discord
DISCORD_TOKEN=your_discord_token

# ... (keep all existing vars)
```

4. **Copy to actual `.env`**

```bash
cp .env.example .env

# Edit .env with real keys
nano .env  # or your preferred editor
```

5. **Create setup script**

```bash
# scripts/setup_dev_env.sh
#!/bin/bash
set -e

echo "üöÄ Setting up development environment..."

# Check Python version
python_version=$(python3 --version | cut -d' ' -f2 | cut -d'.' -f1,2)
if (( $(echo "$python_version < 3.11" | bc -l) )); then
    echo "‚ùå Python 3.11+ required (found $python_version)"
    exit 1
fi
echo "‚úÖ Python $python_version"

# Check Poetry
if ! command -v poetry &> /dev/null; then
    echo "‚ùå Poetry not found. Install from https://python-poetry.org/"
    exit 1
fi
echo "‚úÖ Poetry installed"

# Install dependencies
echo "üì¶ Installing dependencies..."
poetry install

# Check Redis
if ! redis-cli ping &> /dev/null; then
    echo "‚ö†Ô∏è  Redis not running. Start with: docker run -d -p 6379:6379 redis:7-alpine"
else
    echo "‚úÖ Redis connected"
fi

# Check environment
if [ ! -f .env ]; then
    echo "‚ö†Ô∏è  .env file not found. Copying from .env.example..."
    cp .env.example .env
    echo "üìù Edit .env with your API keys"
else
    echo "‚úÖ .env file exists"
fi

# Disable Ollama
if systemctl is-active --quiet ollama; then
    echo "‚ö†Ô∏è  Ollama is running. Disabling..."
    sudo systemctl stop ollama
    sudo systemctl disable ollama
    echo "‚úÖ Ollama disabled"
fi

echo ""
echo "‚úÖ Setup complete!"
echo ""
echo "Next steps:"
echo "1. Edit .env with your API keys"
echo "2. Start Redis: docker run -d -p 6379:6379 redis:7-alpine"
echo "3. Run tests: poetry run pytest"
echo "4. Start development: poetry run python -m catalyst_bot"
```

Make executable:
```bash
chmod +x scripts/setup_dev_env.sh
```

#### Acceptance Criteria

- [ ] `poetry install` succeeds with no errors
- [ ] `.env` file exists with API keys
- [ ] Redis running and accessible
- [ ] Ollama disabled (GPU idle)
- [ ] Setup script runs successfully

#### Testing

```bash
# Run setup
./scripts/setup_dev_env.sh

# Verify dependencies
poetry run python -c "import google.generativeai; import anthropic; print('‚úÖ LLM providers imported')"

# Verify Redis
poetry run python -c "import redis; r = redis.Redis(); r.ping(); print('‚úÖ Redis connected')"
```

#### Documentation

Create `docs/guides/SETUP.md`:

```markdown
# Development Setup Guide

## Prerequisites
- Python 3.11+
- Poetry
- Redis (local or cloud)
- API keys (Gemini, Claude)

## Quick Start
\`\`\`bash
./scripts/setup_dev_env.sh
\`\`\`

## Manual Setup
[Include manual steps if script fails]

## Verification
[Include test commands]
```

---

### Ticket 1.2: LLM Service Core Interface

**Priority**: Critical
**Estimated Time**: 6 hours
**Dependencies**: Ticket 1.1

#### Context

This is the **heart of the new architecture**. Every feature (SEC digester, sentiment analysis, news classification) will use this interface.

**Key Design Decisions**:
- Async-only (use `async def` and `await`)
- Pydantic for type safety
- Provider-agnostic (caller doesn't know if using Gemini or Claude)
- Observable (log all requests, track costs)

#### Implementation Steps

1. **Create base models**

```python
# src/catalyst_bot/services/llm/models.py
"""
Data models for LLM service.

These models define the contract between callers and the LLM service.
"""

from enum import Enum
from typing import Any, Optional
from pydantic import BaseModel, Field


class TaskComplexity(str, Enum):
    """Task complexity levels for routing."""
    SIMPLE = "simple"       # Local or Flash Lite
    MEDIUM = "medium"       # Gemini Flash
    COMPLEX = "complex"     # Gemini Pro
    CRITICAL = "critical"   # Claude Opus


class OutputFormat(str, Enum):
    """Supported output formats."""
    TEXT = "text"           # Free-form text
    JSON = "json"           # Structured JSON
    PYDANTIC = "pydantic"   # Pydantic model validation


class LLMRequest(BaseModel):
    """
    Standardized LLM request format.

    All features use this to interact with LLM service.

    Example:
        request = LLMRequest(
            prompt="Analyze this 8-K filing...",
            complexity=TaskComplexity.MEDIUM,
            output_format=OutputFormat.JSON
        )
    """
    # Core fields
    prompt: str = Field(..., description="The prompt to send to LLM")
    system_prompt: Optional[str] = Field(None, description="System prompt (optional)")

    # Routing hints
    complexity: TaskComplexity = Field(
        default=TaskComplexity.MEDIUM,
        description="Task complexity for routing"
    )
    max_tokens: int = Field(default=1024, ge=1, le=100000)
    temperature: float = Field(default=0.7, ge=0.0, le=2.0)

    # Output control
    output_format: OutputFormat = Field(default=OutputFormat.TEXT)
    output_schema: Optional[type[BaseModel]] = Field(
        None,
        description="Pydantic model for validation (if output_format=PYDANTIC)"
    )

    # Optimization
    enable_cache: bool = Field(default=True, description="Enable semantic caching")
    cache_ttl_seconds: int = Field(default=86400, description="Cache TTL (24 hours)")
    compress_prompt: bool = Field(default=True, description="Enable prompt compression")

    # Metadata
    request_id: Optional[str] = Field(None, description="Unique request ID")
    user_id: Optional[str] = Field(None, description="User/ticker ID for scoping")
    feature_name: str = Field(default="unknown", description="Calling feature name")

    # Advanced
    fallback_on_error: bool = Field(default=True, description="Try fallback provider on error")
    max_retries: int = Field(default=3, ge=0, le=5)
    timeout_seconds: float = Field(default=30.0, gt=0.0, le=300.0)

    class Config:
        use_enum_values = True


class LLMResponse(BaseModel):
    """
    Standardized LLM response format.

    Contains both the response text and extensive metadata for monitoring.
    """
    # Core response
    text: str = Field(..., description="Generated text")
    parsed_output: Optional[Any] = Field(None, description="Parsed JSON/Pydantic output")

    # Metadata
    provider: str = Field(..., description="Provider used (gemini, claude, etc.)")
    model: str = Field(..., description="Specific model (gemini-2.5-flash, etc.)")
    cached: bool = Field(default=False, description="Was response from cache?")

    # Performance
    latency_ms: float = Field(..., description="Total request latency")
    tokens_input: int = Field(default=0, description="Input tokens")
    tokens_output: int = Field(default=0, description="Output tokens")
    cost_usd: float = Field(default=0.0, description="Estimated cost in USD")

    # Quality
    confidence: float = Field(default=1.0, ge=0.0, le=1.0)
    finish_reason: str = Field(default="complete", description="Completion reason")

    # Debugging
    cache_key: Optional[str] = Field(None, description="Cache key (if cached)")
    prompt_compressed: bool = Field(default=False, description="Was prompt compressed?")
    retries: int = Field(default=0, description="Number of retries")

    class Config:
        json_schema_extra = {
            "example": {
                "text": "Apple reports strong Q4 earnings...",
                "provider": "gemini",
                "model": "gemini-2.5-flash",
                "cached": False,
                "latency_ms": 324.5,
                "tokens_input": 450,
                "tokens_output": 120,
                "cost_usd": 0.000045
            }
        }


class LLMServiceError(Exception):
    """Base exception for LLM service errors."""
    pass


class ProviderError(LLMServiceError):
    """Error from LLM provider (API error, rate limit, etc.)."""
    pass


class ValidationError(LLMServiceError):
    """Error validating request or response."""
    pass
```

2. **Create main service interface**

```python
# src/catalyst_bot/services/llm/service.py
"""
Unified LLM Service - Central orchestrator for all LLM operations.

This is the main entry point for all LLM interactions across Catalyst-Bot.

Usage:
    # Initialize service (singleton recommended)
    llm_service = LLMService()

    # Simple query
    request = LLMRequest(
        prompt="Classify this news headline...",
        complexity=TaskComplexity.SIMPLE
    )
    response = await llm_service.query(request)

    # Structured output
    request = LLMRequest(
        prompt="Analyze this filing...",
        output_format=OutputFormat.PYDANTIC,
        output_schema=FilingAnalysis
    )
    response = await llm_service.query(request)
    analysis = response.parsed_output  # FilingAnalysis instance
"""

import asyncio
import time
from typing import List, Optional
import structlog

from .models import LLMRequest, LLMResponse, LLMServiceError
from .router import ModelRouter
from .cache import SemanticCache
from .monitor import UsageMonitor
from .config import LLMServiceConfig

logger = structlog.get_logger()


class LLMService:
    """
    Unified LLM service hub.

    Provides centralized access to multiple LLM providers with:
    - Intelligent routing based on task complexity
    - Semantic caching (60-80% hit rate)
    - Cost tracking and budget enforcement
    - Automatic failover and retry logic
    """

    def __init__(self, config: Optional[LLMServiceConfig] = None):
        """Initialize LLM service with configuration."""
        self.config = config or LLMServiceConfig.from_env()

        # Components (lazy initialization)
        self._router: Optional[ModelRouter] = None
        self._cache: Optional[SemanticCache] = None
        self._monitor: Optional[UsageMonitor] = None
        self._providers: dict = {}

        self._initialized = False
        logger.info("llm_service.init", config=self.config.dict())

    async def initialize(self):
        """Initialize service components (call once at startup)."""
        if self._initialized:
            return

        logger.info("llm_service.initializing")

        # Initialize components
        from .router import ModelRouter
        from .cache import SemanticCache
        from .monitor import UsageMonitor

        self._router = ModelRouter(config=self.config)
        self._cache = SemanticCache(config=self.config)
        self._monitor = UsageMonitor(config=self.config)

        # Initialize providers (lazy load)
        await self._router.initialize()
        await self._cache.initialize()

        self._initialized = True
        logger.info("llm_service.initialized")

    async def query(self, request: LLMRequest) -> LLMResponse:
        """
        Execute LLM query with intelligent routing and optimization.

        This is the main entry point for all LLM operations.

        Args:
            request: Standardized LLM request

        Returns:
            LLMResponse with result and metadata

        Raises:
            LLMServiceError: On unrecoverable errors

        Example:
            request = LLMRequest(
                prompt="Analyze: revenue grew 25% YoY",
                complexity=TaskComplexity.SIMPLE,
                feature_name="sentiment_analysis"
            )
            response = await llm.query(request)
            print(response.text)
        """
        if not self._initialized:
            await self.initialize()

        start_time = time.time()

        logger.info(
            "llm_service.query.start",
            feature=request.feature_name,
            complexity=request.complexity,
            prompt_len=len(request.prompt)
        )

        try:
            # 1. Check cache (if enabled)
            if request.enable_cache:
                cached = await self._check_cache(request)
                if cached:
                    logger.info("llm_service.query.cache_hit")
                    return cached

            # 2. Route to appropriate provider
            provider_name, model_name = self._router.select_provider(request)
            logger.info("llm_service.query.routed", provider=provider_name, model=model_name)

            # 3. Execute query (with retry logic)
            response = await self._execute_with_retry(request, provider_name, model_name)

            # 4. Cache response (if enabled)
            if request.enable_cache:
                await self._save_to_cache(request, response)

            # 5. Track usage
            await self._monitor.track_usage(request, response)

            # 6. Return response
            elapsed_ms = (time.time() - start_time) * 1000
            response.latency_ms = elapsed_ms

            logger.info(
                "llm_service.query.complete",
                provider=response.provider,
                latency_ms=elapsed_ms,
                cost_usd=response.cost_usd,
                cached=response.cached
            )

            return response

        except Exception as e:
            elapsed_ms = (time.time() - start_time) * 1000
            logger.error(
                "llm_service.query.error",
                error=str(e),
                latency_ms=elapsed_ms
            )
            raise LLMServiceError(f"Query failed: {e}") from e

    async def query_batch(self, requests: List[LLMRequest]) -> List[LLMResponse]:
        """
        Process multiple requests efficiently with batching.

        Executes requests concurrently while respecting rate limits.

        Args:
            requests: List of LLM requests

        Returns:
            List of responses (same order as requests)

        Example:
            requests = [
                LLMRequest(prompt="Classify: ...") for _ in range(10)
            ]
            responses = await llm.query_batch(requests)
        """
        logger.info("llm_service.query_batch.start", count=len(requests))

        # Execute concurrently with semaphore for rate limiting
        semaphore = asyncio.Semaphore(self.config.max_concurrent_requests)

        async def query_with_limit(req):
            async with semaphore:
                return await self.query(req)

        responses = await asyncio.gather(
            *[query_with_limit(req) for req in requests],
            return_exceptions=True
        )

        # Convert exceptions to error responses
        results = []
        for i, response in enumerate(responses):
            if isinstance(response, Exception):
                logger.error("llm_service.query_batch.error", index=i, error=str(response))
                # Return error response
                results.append(LLMResponse(
                    text=f"Error: {response}",
                    provider="error",
                    model="error",
                    latency_ms=0.0,
                    finish_reason="error"
                ))
            else:
                results.append(response)

        logger.info("llm_service.query_batch.complete", count=len(results))
        return results

    async def estimate_cost(self, request: LLMRequest) -> float:
        """
        Estimate cost in USD before executing.

        Useful for budget checks before expensive operations.

        Args:
            request: LLM request to estimate

        Returns:
            Estimated cost in USD

        Example:
            request = LLMRequest(prompt=long_filing_text)
            cost = await llm.estimate_cost(request)
            if cost < 0.10:
                response = await llm.query(request)
        """
        if not self._initialized:
            await self.initialize()

        # Select provider/model
        provider_name, model_name = self._router.select_provider(request)

        # Estimate tokens
        estimated_tokens = len(request.prompt) // 4  # Rough estimate

        # Get cost per token
        cost_per_million = self._router.get_cost_per_million(provider_name, model_name)
        estimated_cost = (estimated_tokens / 1_000_000) * cost_per_million

        return estimated_cost

    def get_stats(self) -> dict:
        """
        Get usage statistics and performance metrics.

        Returns:
            Dict with stats (requests, costs, latency, cache hits, etc.)
        """
        if not self._initialized:
            return {}

        return {
            "monitor": self._monitor.get_stats(),
            "cache": self._cache.get_stats(),
            "router": self._router.get_stats(),
        }

    # ========================================================================
    # Private methods
    # ========================================================================

    async def _check_cache(self, request: LLMRequest) -> Optional[LLMResponse]:
        """Check semantic cache for similar request."""
        return await self._cache.get(request)

    async def _save_to_cache(self, request: LLMRequest, response: LLMResponse):
        """Save response to cache."""
        await self._cache.set(request, response, ttl=request.cache_ttl_seconds)

    async def _execute_with_retry(
        self,
        request: LLMRequest,
        provider_name: str,
        model_name: str
    ) -> LLMResponse:
        """Execute query with automatic retry and fallback logic."""
        max_retries = request.max_retries
        retry_count = 0

        while retry_count <= max_retries:
            try:
                # Get provider instance
                provider = await self._router.get_provider(provider_name)

                # Execute query
                response = await provider.query(
                    prompt=request.prompt,
                    system_prompt=request.system_prompt,
                    model=model_name,
                    max_tokens=request.max_tokens,
                    temperature=request.temperature
                )

                # Validate output format
                if request.output_format == OutputFormat.PYDANTIC and request.output_schema:
                    response = await self._validate_pydantic(response, request.output_schema)

                response.retries = retry_count
                return response

            except Exception as e:
                retry_count += 1
                logger.warning(
                    "llm_service.retry",
                    retry=retry_count,
                    max_retries=max_retries,
                    error=str(e)
                )

                if retry_count > max_retries:
                    # Try fallback provider if enabled
                    if request.fallback_on_error:
                        fallback_provider, fallback_model = self._router.get_fallback()
                        if fallback_provider != provider_name:
                            logger.info("llm_service.fallback", provider=fallback_provider)
                            provider_name = fallback_provider
                            model_name = fallback_model
                            retry_count = 0  # Reset retries for fallback
                            continue

                    raise

                # Exponential backoff
                await asyncio.sleep(2 ** retry_count)

        raise LLMServiceError("Max retries exceeded")

    async def _validate_pydantic(self, response: LLMResponse, schema: type[BaseModel]) -> LLMResponse:
        """Validate and parse Pydantic output."""
        import json

        try:
            # Parse JSON from response
            data = json.loads(response.text)

            # Validate with Pydantic
            parsed = schema(**data)
            response.parsed_output = parsed

            return response

        except Exception as e:
            raise LLMServiceError(f"Failed to validate Pydantic output: {e}") from e


# ============================================================================
# Singleton instance (recommended pattern)
# ============================================================================

_llm_service_instance: Optional[LLMService] = None


async def get_llm_service() -> LLMService:
    """
    Get singleton LLM service instance.

    Usage:
        llm = await get_llm_service()
        response = await llm.query(request)
    """
    global _llm_service_instance

    if _llm_service_instance is None:
        _llm_service_instance = LLMService()
        await _llm_service_instance.initialize()

    return _llm_service_instance
```

#### Acceptance Criteria

- [ ] `LLMRequest` and `LLMResponse` models defined
- [ ] `LLMService` class with `query()` method
- [ ] `query_batch()` for concurrent requests
- [ ] `estimate_cost()` for budget checks
- [ ] Singleton pattern implemented
- [ ] Type hints on all methods
- [ ] Docstrings with examples

#### Testing

```python
# tests/services/llm/test_service.py
import pytest
from catalyst_bot.services.llm import LLMService, LLMRequest, TaskComplexity

@pytest.mark.asyncio
async def test_llm_service_initialization():
    """Test service initializes correctly."""
    service = LLMService()
    await service.initialize()
    assert service._initialized

@pytest.mark.asyncio
async def test_llm_query_simple():
    """Test simple LLM query."""
    service = LLMService()
    await service.initialize()

    request = LLMRequest(
        prompt="Say 'Hello, World!'",
        complexity=TaskComplexity.SIMPLE,
        max_tokens=20
    )

    response = await service.query(request)

    assert "hello" in response.text.lower()
    assert response.provider in ["gemini", "claude"]
    assert response.latency_ms > 0
    assert response.cost_usd >= 0

@pytest.mark.asyncio
async def test_cost_estimation():
    """Test cost estimation."""
    service = LLMService()
    await service.initialize()

    request = LLMRequest(
        prompt="A" * 1000,  # Long prompt
        complexity=TaskComplexity.COMPLEX
    )

    cost = await service.estimate_cost(request)
    assert cost > 0
    assert cost < 1.0  # Should be < $1
```

Run tests:
```bash
poetry run pytest tests/services/llm/test_service.py -v
```

---

### Ticket 1.3: Provider Adapters (Gemini + Claude)

**Priority**: Critical
**Estimated Time**: 6 hours
**Dependencies**: Ticket 1.2

#### Context

Provider adapters wrap the actual API clients (Gemini, Claude) and provide a **uniform interface**. The router selects a provider, but the calling code doesn't need to know which one.

**Key Pattern**: Abstract base class + concrete implementations

#### Implementation Steps

1. **Create base provider interface**

```python
# src/catalyst_bot/services/llm/providers/base.py
"""
Abstract base class for LLM providers.

All provider implementations (Gemini, Claude, etc.) must implement this interface.
"""

from abc import ABC, abstractmethod
from typing import Optional
import structlog

from ..models import LLMResponse

logger = structlog.get_logger()


class BaseLLMProvider(ABC):
    """
    Abstract base class for LLM providers.

    Subclasses must implement query() method and provide pricing info.
    """

    def __init__(self, api_key: str, **kwargs):
        """Initialize provider with API key."""
        self.api_key = api_key
        self.config = kwargs
        logger.info(f"{self.__class__.__name__}.init")

    @abstractmethod
    async def query(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        model: str = None,
        max_tokens: int = 1024,
        temperature: float = 0.7,
        **kwargs
    ) -> LLMResponse:
        """
        Execute LLM query.

        Args:
            prompt: User prompt
            system_prompt: System prompt (optional)
            model: Specific model name
            max_tokens: Max tokens to generate
            temperature: Sampling temperature

        Returns:
            LLMResponse with text and metadata

        Raises:
            ProviderError: On API errors
        """
        raise NotImplementedError

    @abstractmethod
    def get_cost_per_million(self, model: str) -> float:
        """
        Get cost per million tokens for model.

        Args:
            model: Model name

        Returns:
            Cost in USD per 1M input tokens
        """
        raise NotImplementedError

    @abstractmethod
    def list_models(self) -> list[str]:
        """
        List available models for this provider.

        Returns:
            List of model names
        """
        raise NotImplementedError

    def estimate_tokens(self, text: str) -> int:
        """
        Estimate token count for text.

        Simple heuristic: ~4 chars per token.
        Override for provider-specific tokenizers.

        Args:
            text: Text to estimate

        Returns:
            Estimated token count
        """
        return len(text) // 4

    def calculate_cost(self, tokens_input: int, tokens_output: int, model: str) -> float:
        """
        Calculate cost for request.

        Args:
            tokens_input: Input tokens
            tokens_output: Output tokens
            model: Model name

        Returns:
            Cost in USD
        """
        cost_per_million = self.get_cost_per_million(model)
        total_tokens = tokens_input + tokens_output
        return (total_tokens / 1_000_000) * cost_per_million
```

2. **Implement Gemini provider**

```python
# src/catalyst_bot/services/llm/providers/gemini.py
"""
Google Gemini API provider.

Supports:
- gemini-2.0-flash-lite
- gemini-2.5-flash
- gemini-2.5-pro
"""

import time
from typing import Optional
import structlog
import google.generativeai as genai

from .base import BaseLLMProvider
from ..models import LLMResponse, ProviderError

logger = structlog.get_logger()


# Pricing (as of 2025-01)
GEMINI_PRICING = {
    "gemini-2.0-flash-lite": 0.02,      # $0.02 per 1M tokens
    "gemini-2.5-flash": 0.075,          # $0.075 per 1M tokens
    "gemini-1.5-flash-002": 0.075,
    "gemini-2.5-pro": 1.25,             # $1.25 per 1M tokens
    "gemini-1.5-pro-002": 1.25,
}


class GeminiProvider(BaseLLMProvider):
    """
    Google Gemini API provider.

    Uses google-generativeai Python SDK.

    Rate Limits (Free Tier):
    - 1,500 requests/day
    - 60 requests/minute

    Example:
        provider = GeminiProvider(api_key="your_key")
        response = await provider.query(
            prompt="Explain quantum physics",
            model="gemini-2.5-flash"
        )
    """

    def __init__(self, api_key: str, **kwargs):
        super().__init__(api_key, **kwargs)

        # Configure SDK
        genai.configure(api_key=api_key)

        # Default model
        self.default_model = kwargs.get("default_model", "gemini-2.5-flash")

        logger.info("gemini.initialized", default_model=self.default_model)

    async def query(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        model: str = None,
        max_tokens: int = 1024,
        temperature: float = 0.7,
        **kwargs
    ) -> LLMResponse:
        """Execute Gemini API query."""
        model = model or self.default_model
        start_time = time.time()

        logger.info("gemini.query.start", model=model, prompt_len=len(prompt))

        try:
            # Create model instance
            gemini_model = genai.GenerativeModel(
                model_name=model,
                generation_config={
                    "max_output_tokens": max_tokens,
                    "temperature": temperature,
                }
            )

            # Build prompt
            full_prompt = prompt
            if system_prompt:
                full_prompt = f"{system_prompt}\n\n{prompt}"

            # Generate
            response = await gemini_model.generate_content_async(full_prompt)

            # Extract response text
            text = response.text

            # Calculate tokens (Gemini doesn't provide token count in response)
            tokens_input = self.estimate_tokens(full_prompt)
            tokens_output = self.estimate_tokens(text)
            cost = self.calculate_cost(tokens_input, tokens_output, model)

            elapsed_ms = (time.time() - start_time) * 1000

            logger.info(
                "gemini.query.complete",
                latency_ms=elapsed_ms,
                tokens_input=tokens_input,
                tokens_output=tokens_output,
                cost_usd=cost
            )

            return LLMResponse(
                text=text,
                provider="gemini",
                model=model,
                cached=False,
                latency_ms=elapsed_ms,
                tokens_input=tokens_input,
                tokens_output=tokens_output,
                cost_usd=cost,
                finish_reason=response.candidates[0].finish_reason.name if response.candidates else "unknown"
            )

        except Exception as e:
            logger.error("gemini.query.error", error=str(e))
            raise ProviderError(f"Gemini API error: {e}") from e

    def get_cost_per_million(self, model: str) -> float:
        """Get cost per million tokens."""
        return GEMINI_PRICING.get(model, 0.075)  # Default to Flash pricing

    def list_models(self) -> list[str]:
        """List available Gemini models."""
        return list(GEMINI_PRICING.keys())
```

3. **Implement Claude provider**

```python
# src/catalyst_bot/services/llm/providers/claude.py
"""
Anthropic Claude API provider.

Supports:
- claude-3-5-sonnet-20241022
- claude-3-haiku-20240307
- claude-3-opus-20240229
"""

import time
from typing import Optional
import structlog
from anthropic import AsyncAnthropic

from .base import BaseLLMProvider
from ..models import LLMResponse, ProviderError

logger = structlog.get_logger()


# Pricing (as of 2025-01)
CLAUDE_PRICING = {
    "claude-3-5-sonnet-20241022": 3.00,     # $3.00 per 1M input tokens
    "claude-3-haiku-20240307": 0.25,        # $0.25 per 1M tokens
    "claude-3-opus-20240229": 15.00,        # $15.00 per 1M tokens
}


class ClaudeProvider(BaseLLMProvider):
    """
    Anthropic Claude API provider.

    Uses anthropic Python SDK.

    Rate Limits:
    - Varies by tier (check console.anthropic.com)

    Example:
        provider = ClaudeProvider(api_key="your_key")
        response = await provider.query(
            prompt="Explain quantum physics",
            model="claude-3-5-sonnet-20241022"
        )
    """

    def __init__(self, api_key: str, **kwargs):
        super().__init__(api_key, **kwargs)

        # Initialize async client
        self.client = AsyncAnthropic(api_key=api_key)

        # Default model
        self.default_model = kwargs.get("default_model", "claude-3-5-sonnet-20241022")

        logger.info("claude.initialized", default_model=self.default_model)

    async def query(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        model: str = None,
        max_tokens: int = 1024,
        temperature: float = 0.7,
        **kwargs
    ) -> LLMResponse:
        """Execute Claude API query."""
        model = model or self.default_model
        start_time = time.time()

        logger.info("claude.query.start", model=model, prompt_len=len(prompt))

        try:
            # Build messages
            messages = [{"role": "user", "content": prompt}]

            # Call API
            response = await self.client.messages.create(
                model=model,
                max_tokens=max_tokens,
                temperature=temperature,
                system=system_prompt if system_prompt else None,
                messages=messages
            )

            # Extract text
            text = response.content[0].text

            # Get token counts (Claude provides these!)
            tokens_input = response.usage.input_tokens
            tokens_output = response.usage.output_tokens
            cost = self.calculate_cost(tokens_input, tokens_output, model)

            elapsed_ms = (time.time() - start_time) * 1000

            logger.info(
                "claude.query.complete",
                latency_ms=elapsed_ms,
                tokens_input=tokens_input,
                tokens_output=tokens_output,
                cost_usd=cost
            )

            return LLMResponse(
                text=text,
                provider="claude",
                model=model,
                cached=False,
                latency_ms=elapsed_ms,
                tokens_input=tokens_input,
                tokens_output=tokens_output,
                cost_usd=cost,
                finish_reason=response.stop_reason
            )

        except Exception as e:
            logger.error("claude.query.error", error=str(e))
            raise ProviderError(f"Claude API error: {e}") from e

    def get_cost_per_million(self, model: str) -> float:
        """Get cost per million tokens."""
        return CLAUDE_PRICING.get(model, 3.00)  # Default to Sonnet pricing

    def list_models(self) -> list[str]:
        """List available Claude models."""
        return list(CLAUDE_PRICING.keys())
```

4. **Create provider factory**

```python
# src/catalyst_bot/services/llm/providers/__init__.py
"""
LLM provider implementations.

Supported providers:
- Gemini (Google)
- Claude (Anthropic)
"""

from .base import BaseLLMProvider
from .gemini import GeminiProvider
from .claude import ClaudeProvider

__all__ = [
    "BaseLLMProvider",
    "GeminiProvider",
    "ClaudeProvider",
    "get_provider",
]


def get_provider(provider_name: str, api_key: str, **kwargs) -> BaseLLMProvider:
    """
    Factory function to get provider instance.

    Args:
        provider_name: Provider name ("gemini", "claude")
        api_key: API key for provider
        **kwargs: Additional configuration

    Returns:
        Provider instance

    Raises:
        ValueError: If provider not found

    Example:
        provider = get_provider("gemini", api_key="your_key")
        response = await provider.query("Hello, world!")
    """
    providers = {
        "gemini": GeminiProvider,
        "claude": ClaudeProvider,
    }

    provider_class = providers.get(provider_name.lower())
    if not provider_class:
        raise ValueError(f"Unknown provider: {provider_name}")

    return provider_class(api_key=api_key, **kwargs)
```

#### Acceptance Criteria

- [ ] `BaseLLMProvider` abstract class defined
- [ ] `GeminiProvider` implemented with pricing
- [ ] `ClaudeProvider` implemented with pricing
- [ ] Provider factory function
- [ ] Token estimation method
- [ ] Cost calculation method
- [ ] Error handling for API failures

#### Testing

```python
# scripts/test_llm_providers.py
"""Test script to verify LLM provider connectivity."""

import asyncio
import os
from catalyst_bot.services.llm.providers import get_provider

async def test_gemini():
    """Test Gemini provider."""
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        print("‚ùå GEMINI_API_KEY not set")
        return

    print("Testing Gemini provider...")
    provider = get_provider("gemini", api_key=api_key)

    response = await provider.query(
        prompt="Say 'Gemini test successful' and nothing else.",
        model="gemini-2.5-flash",
        max_tokens=20
    )

    print(f"‚úÖ Response: {response.text}")
    print(f"   Model: {response.model}")
    print(f"   Latency: {response.latency_ms:.0f}ms")
    print(f"   Cost: ${response.cost_usd:.6f}")

async def test_claude():
    """Test Claude provider."""
    api_key = os.getenv("ANTHROPIC_API_KEY")
    if not api_key:
        print("‚ùå ANTHROPIC_API_KEY not set")
        return

    print("\nTesting Claude provider...")
    provider = get_provider("claude", api_key=api_key)

    response = await provider.query(
        prompt="Say 'Claude test successful' and nothing else.",
        model="claude-3-5-sonnet-20241022",
        max_tokens=20
    )

    print(f"‚úÖ Response: {response.text}")
    print(f"   Model: {response.model}")
    print(f"   Latency: {response.latency_ms:.0f}ms")
    print(f"   Cost: ${response.cost_usd:.6f}")

async def main():
    """Run all tests."""
    await test_gemini()
    await test_claude()
    print("\n‚úÖ All providers working!")

if __name__ == "__main__":
    asyncio.run(main())
```

Run test:
```bash
poetry run python scripts/test_llm_providers.py
```

Expected output:
```
Testing Gemini provider...
‚úÖ Response: Gemini test successful
   Model: gemini-2.5-flash
   Latency: 324ms
   Cost: $0.000012

Testing Claude provider...
‚úÖ Response: Claude test successful
   Model: claude-3-5-sonnet-20241022
   Latency: 456ms
   Cost: $0.000045

‚úÖ All providers working!
```

---

### Ticket 1.4: Intelligent Router

**Priority**: High
**Estimated Time**: 4 hours
**Dependencies**: Ticket 1.3

#### Context

The router decides **which model to use** based on task complexity. This is where cost optimization happens.

**Routing Logic**:
- Simple tasks ‚Üí Gemini Flash Lite (cheapest)
- Medium tasks ‚Üí Gemini Flash (best value)
- Complex tasks ‚Üí Gemini Pro (deep analysis)
- Critical tasks ‚Üí Claude Sonnet (highest accuracy)

#### Implementation

```python
# src/catalyst_bot/services/llm/router.py
"""
Intelligent model router with complexity-based selection.

Automatically selects the best model for each task based on:
- Task complexity (simple/medium/complex/critical)
- Content length
- Required accuracy
- Cost budget
- Provider availability
"""

import random
from typing import Tuple, Dict
import structlog

from .models import LLMRequest, TaskComplexity
from .providers import get_provider, BaseLLMProvider
from .config import LLMServiceConfig

logger = structlog.get_logger()


class ModelRouter:
    """
    Routes requests to optimal model based on task characteristics.

    Routing Strategy:
    - 60% ‚Üí Gemini Flash Lite (simple tasks)
    - 30% ‚Üí Gemini Flash (medium tasks)
    - 8% ‚Üí Gemini Pro (complex tasks)
    - 2% ‚Üí Claude Sonnet (critical tasks)
    """

    # Routing decision matrix
    ROUTING_RULES = {
        TaskComplexity.SIMPLE: [
            ("gemini_flash_lite", 0.70),     # 70% to Flash Lite
            ("gemini_flash", 0.25),          # 25% to Flash
            ("gemini_pro", 0.05),            # 5% fallback to Pro
        ],
        TaskComplexity.MEDIUM: [
            ("gemini_flash", 0.90),          # 90% to Flash
            ("gemini_pro", 0.10),            # 10% to Pro
        ],
        TaskComplexity.COMPLEX: [
            ("gemini_pro", 0.80),            # 80% to Pro
            ("claude_sonnet", 0.20),         # 20% to Claude
        ],
        TaskComplexity.CRITICAL: [
            ("claude_sonnet", 1.0),          # 100% to Claude
        ],
    }

    # Model name mapping
    MODEL_NAMES = {
        "gemini_flash_lite": "gemini-2.0-flash-lite",
        "gemini_flash": "gemini-2.5-flash",
        "gemini_pro": "gemini-2.5-pro",
        "claude_sonnet": "claude-3-5-sonnet-20241022",
        "claude_haiku": "claude-3-haiku-20240307",
    }

    def __init__(self, config: LLMServiceConfig):
        self.config = config
        self._providers: Dict[str, BaseLLMProvider] = {}
        self._stats = {
            "requests_by_provider": {},
            "requests_by_complexity": {},
        }

    async def initialize(self):
        """Initialize provider instances."""
        # Gemini provider
        if self.config.gemini_api_key:
            from .providers.gemini import GeminiProvider
            self._providers["gemini"] = GeminiProvider(
                api_key=self.config.gemini_api_key
            )
            logger.info("router.provider.initialized", provider="gemini")

        # Claude provider
        if self.config.anthropic_api_key:
            from .providers.claude import ClaudeProvider
            self._providers["claude"] = ClaudeProvider(
                api_key=self.config.anthropic_api_key
            )
            logger.info("router.provider.initialized", provider="claude")

    def select_provider(self, request: LLMRequest) -> Tuple[str, str]:
        """
        Select optimal provider and model for request.

        Returns:
            (provider_name, model_name)

        Example:
            provider, model = router.select_provider(request)
            # ("gemini", "gemini-2.5-flash")
        """
        # Auto-detect complexity if needed
        complexity = request.complexity
        if complexity == TaskComplexity.MEDIUM:
            complexity = self._auto_detect_complexity(request)

        # Get routing rules for complexity
        rules = self.ROUTING_RULES.get(complexity, self.ROUTING_RULES[TaskComplexity.MEDIUM])

        # Select based on probability distribution
        rand = random.random()
        cumulative = 0.0

        for model_key, prob in rules:
            cumulative += prob
            if rand < cumulative:
                provider_name = self._get_provider_name(model_key)
                model_name = self.MODEL_NAMES[model_key]

                # Track stats
                self._stats["requests_by_provider"][provider_name] = \
                    self._stats["requests_by_provider"].get(provider_name, 0) + 1
                self._stats["requests_by_complexity"][complexity.value] = \
                    self._stats["requests_by_complexity"].get(complexity.value, 0) + 1

                logger.info(
                    "router.select",
                    complexity=complexity.value,
                    provider=provider_name,
                    model=model_name
                )

                return provider_name, model_name

        # Fallback to first option
        model_key = rules[0][0]
        provider_name = self._get_provider_name(model_key)
        model_name = self.MODEL_NAMES[model_key]
        return provider_name, model_name

    async def get_provider(self, provider_name: str) -> BaseLLMProvider:
        """Get provider instance by name."""
        provider = self._providers.get(provider_name)
        if not provider:
            raise ValueError(f"Provider not initialized: {provider_name}")
        return provider

    def get_fallback(self) -> Tuple[str, str]:
        """
        Get fallback provider and model.

        Returns:
            (provider_name, model_name) for fallback
        """
        # Use Claude Sonnet as ultimate fallback
        return "claude", self.MODEL_NAMES["claude_sonnet"]

    def get_cost_per_million(self, provider_name: str, model_name: str) -> float:
        """Get cost per million tokens for model."""
        provider = self._providers.get(provider_name)
        if provider:
            return provider.get_cost_per_million(model_name)
        return 0.0

    def get_stats(self) -> dict:
        """Get routing statistics."""
        return self._stats

    # ========================================================================
    # Private methods
    # ========================================================================

    def _auto_detect_complexity(self, request: LLMRequest) -> TaskComplexity:
        """
        Automatically detect task complexity based on request characteristics.

        Heuristics:
        - Prompt length
        - Keywords ("analyze", "extract", "compare")
        - Output format (Pydantic = more complex)
        - Max tokens requested
        """
        score = 0.0

        # Length-based scoring
        prompt_len = len(request.prompt)
        if prompt_len > 2000:
            score += 0.4
        elif prompt_len > 500:
            score += 0.2

        # Keyword-based scoring
        complex_keywords = ["analyze", "extract", "summarize", "compare", "evaluate", "explain"]
        text_lower = request.prompt.lower()
        keyword_count = sum(1 for kw in complex_keywords if kw in text_lower)
        score += min(0.3, keyword_count * 0.1)

        # Output format scoring
        if request.output_format == OutputFormat.PYDANTIC:
            if request.output_schema:
                field_count = len(request.output_schema.__fields__)
                if field_count > 5:
                    score += 0.2

        # Token limit scoring
        if request.max_tokens > 2048:
            score += 0.1

        # Map score to complexity
        if score < 0.3:
            return TaskComplexity.SIMPLE
        elif score < 0.7:
            return TaskComplexity.MEDIUM
        else:
            return TaskComplexity.COMPLEX

    def _get_provider_name(self, model_key: str) -> str:
        """Get provider name from model key."""
        if model_key.startswith("gemini"):
            return "gemini"
        elif model_key.startswith("claude"):
            return "claude"
        else:
            return "unknown"
```

#### Acceptance Criteria

- [ ] Router selects models based on complexity
- [ ] Auto-detection of complexity works
- [ ] Fallback provider configured
- [ ] Stats tracking implemented
- [ ] Cost estimation per model

#### Testing

```python
# tests/services/llm/test_router.py
import pytest
from catalyst_bot.services.llm import ModelRouter, LLMRequest, TaskComplexity
from catalyst_bot.services.llm.config import LLMServiceConfig

@pytest.fixture
def router():
    config = LLMServiceConfig.from_env()
    router = ModelRouter(config)
    return router

def test_router_simple_task(router):
    """Test routing for simple task."""
    request = LLMRequest(
        prompt="Hello",
        complexity=TaskComplexity.SIMPLE
    )

    provider, model = router.select_provider(request)

    assert provider == "gemini"
    assert "flash" in model.lower()

def test_router_complex_task(router):
    """Test routing for complex task."""
    request = LLMRequest(
        prompt="Analyze this complex financial document...",
        complexity=TaskComplexity.COMPLEX
    )

    provider, model = router.select_provider(request)

    assert provider in ["gemini", "claude"]
    assert "pro" in model.lower() or "sonnet" in model.lower()

def test_router_auto_detect(router):
    """Test automatic complexity detection."""
    # Long prompt with complex keywords
    request = LLMRequest(
        prompt="Analyze and extract all financial metrics from this 10-K filing, comparing YoY growth..." * 10,
        max_tokens=2048
    )

    complexity = router._auto_detect_complexity(request)

    assert complexity in [TaskComplexity.MEDIUM, TaskComplexity.COMPLEX]
```

---

### Ticket 1.5: Semantic Cache Implementation

**Priority**: High
**Estimated Time**: 6 hours
**Dependencies**: Ticket 1.2

#### Context

Semantic caching is **critical for cost savings**. Instead of exact string matching, we use **embeddings** to find similar prompts.

**Example**:
- Prompt A: "What is AAPL revenue?"
- Prompt B: "Tell me Apple's revenue"
- Traditional cache: MISS (different strings)
- Semantic cache: HIT (same meaning, similarity > 0.95)

**Target**: 70% cache hit rate

#### Implementation

```python
# src/catalyst_bot/services/llm/cache.py
"""
Semantic LLM response caching using Redis and sentence embeddings.

Matches prompts based on semantic similarity rather than exact string matching.
Expected cache hit rate: 60-80% for similar queries.
"""

import hashlib
import json
import time
from typing import Optional, Tuple
import structlog
import redis.asyncio as redis
import numpy as np
from sentence_transformers import SentenceTransformer

from .models import LLMRequest, LLMResponse
from .config import LLMServiceConfig

logger = structlog.get_logger()


class SemanticCache:
    """
    Semantic LLM response caching.

    Caches responses based on semantic similarity of prompts,
    not exact string matching.

    Example:
        "What is AAPL revenue?" and "Tell me Apple's revenue"
        will match despite different wording.

    Backend: Redis with embeddings stored as byte arrays
    Embedding Model: sentence-transformers/all-MiniLM-L6-v2 (fast, good quality)
    """

    def __init__(self, config: LLMServiceConfig):
        self.config = config
        self.redis: Optional[redis.Redis] = None
        self.encoder: Optional[SentenceTransformer] = None

        # Stats
        self.hits = 0
        self.misses = 0
        self.saves = 0

    async def initialize(self):
        """Initialize Redis connection and embedding model."""
        # Connect to Redis
        self.redis = await redis.from_url(
            self.config.redis_url,
            encoding="utf-8",
            decode_responses=False  # We handle bytes for embeddings
        )

        # Test connection
        await self.redis.ping()
        logger.info("cache.redis.connected")

        # Load embedding model (small, fast model)
        self.encoder = SentenceTransformer("all-MiniLM-L6-v2")
        logger.info("cache.encoder.loaded", model="all-MiniLM-L6-v2")

    async def get(self, request: LLMRequest) -> Optional[LLMResponse]:
        """
        Retrieve cached response for semantically similar prompt.

        Args:
            request: LLM request to check

        Returns:
            Cached response if found (similarity > threshold), else None
        """
        if not self.config.cache_enabled:
            return None

        start_time = time.time()

        try:
            # Generate embedding for prompt
            embedding = self.encoder.encode(request.prompt)

            # Search for similar prompts in cache
            # Key pattern: llm:cache:{feature}:{user_id}:*
            cache_pattern = self._get_cache_pattern(request)

            # Scan for matching keys (could use Redis Streams or Sorted Sets for efficiency)
            cursor = 0
            best_match = None
            best_similarity = 0.0

            while True:
                cursor, keys = await self.redis.scan(
                    cursor,
                    match=cache_pattern,
                    count=100
                )

                for key in keys:
                    # Get cached data
                    cached_data = await self.redis.hgetall(key)
                    if not cached_data:
                        continue

                    # Check TTL
                    ttl = await self.redis.ttl(key)
                    if ttl <= 0:
                        continue

                    # Get stored embedding
                    stored_embedding = np.frombuffer(cached_data[b"embedding"], dtype=np.float32)

                    # Calculate similarity
                    similarity = self._cosine_similarity(embedding, stored_embedding)

                    # Check if better match
                    if similarity > best_similarity and similarity >= self.config.cache_similarity_threshold:
                        best_similarity = similarity
                        best_match = cached_data

                if cursor == 0:
                    break

            # Return best match if found
            if best_match:
                self.hits += 1
                response = self._deserialize_response(best_match[b"response"])
                response.cached = True

                elapsed_ms = (time.time() - start_time) * 1000
                logger.info(
                    "cache.hit",
                    similarity=best_similarity,
                    elapsed_ms=elapsed_ms,
                    hit_rate=self.get_hit_rate()
                )

                return response

            self.misses += 1
            elapsed_ms = (time.time() - start_time) * 1000
            logger.debug("cache.miss", elapsed_ms=elapsed_ms)
            return None

        except Exception as e:
            logger.error("cache.get.error", error=str(e))
            return None

    async def set(
        self,
        request: LLMRequest,
        response: LLMResponse,
        ttl: int = None
    ):
        """
        Cache response with semantic embedding.

        Args:
            request: Original request
            response: Response to cache
            ttl: Time-to-live in seconds (default from config)
        """
        if not self.config.cache_enabled:
            return

        ttl = ttl or self.config.cache_ttl_seconds

        try:
            # Generate embedding
            embedding = self.encoder.encode(request.prompt)

            # Generate cache key
            cache_key = self._get_cache_key(request)

            # Serialize response
            response_data = self._serialize_response(response)

            # Store in Redis
            data = {
                b"embedding": embedding.tobytes(),
                b"response": response_data.encode("utf-8"),
                b"prompt_hash": hashlib.sha256(request.prompt.encode()).hexdigest().encode("utf-8"),
                b"created_at": str(time.time()).encode("utf-8"),
            }

            await self.redis.hset(cache_key, mapping=data)
            await self.redis.expire(cache_key, ttl)

            self.saves += 1
            logger.debug("cache.set", key=cache_key, ttl=ttl)

        except Exception as e:
            logger.error("cache.set.error", error=str(e))

    def get_stats(self) -> dict:
        """Get cache statistics."""
        return {
            "hits": self.hits,
            "misses": self.misses,
            "saves": self.saves,
            "hit_rate": self.get_hit_rate(),
        }

    def get_hit_rate(self) -> float:
        """Calculate cache hit rate."""
        total = self.hits + self.misses
        if total == 0:
            return 0.0
        return self.hits / total

    # ========================================================================
    # Private methods
    # ========================================================================

    def _get_cache_key(self, request: LLMRequest) -> str:
        """Generate unique cache key for request."""
        # Hash prompt for uniqueness
        prompt_hash = hashlib.sha256(request.prompt.encode()).hexdigest()[:16]

        # Include feature and user for scoping
        feature = request.feature_name or "default"
        user = request.user_id or "global"

        return f"llm:cache:{feature}:{user}:{prompt_hash}"

    def _get_cache_pattern(self, request: LLMRequest) -> str:
        """Get Redis key pattern for scanning."""
        feature = request.feature_name or "default"
        user = request.user_id or "global"
        return f"llm:cache:{feature}:{user}:*"

    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:
        """Calculate cosine similarity between two embeddings."""
        return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))

    def _serialize_response(self, response: LLMResponse) -> str:
        """Serialize response to JSON string."""
        return response.model_dump_json()

    def _deserialize_response(self, data: bytes) -> LLMResponse:
        """Deserialize response from JSON bytes."""
        return LLMResponse.model_validate_json(data)
```

#### Acceptance Criteria

- [ ] Semantic similarity matching works
- [ ] Cache hit/miss tracked
- [ ] TTL expiration configured
- [ ] Scoped by feature and user
- [ ] Stats reporting

#### Testing

```python
# tests/services/llm/test_cache.py
import pytest
from catalyst_bot.services.llm import SemanticCache, LLMRequest, LLMResponse, TaskComplexity

@pytest.mark.asyncio
async def test_cache_hit():
    """Test cache hit for similar prompts."""
    cache = SemanticCache(config)
    await cache.initialize()

    # First request
    request1 = LLMRequest(prompt="What is Apple's revenue?", feature_name="test")
    response1 = LLMResponse(
        text="Apple's revenue is $394.3B",
        provider="gemini",
        model="gemini-2.5-flash",
        latency_ms=100.0
    )

    # Cache response
    await cache.set(request1, response1)

    # Similar request (different wording)
    request2 = LLMRequest(prompt="Tell me AAPL revenue", feature_name="test")

    # Should hit cache
    cached = await cache.get(request2)

    assert cached is not None
    assert cached.text == response1.text
    assert cached.cached == True

    # Check hit rate
    assert cache.get_hit_rate() > 0

@pytest.mark.asyncio
async def test_cache_miss():
    """Test cache miss for different prompts."""
    cache = SemanticCache(config)
    await cache.initialize()

    request = LLMRequest(prompt="Explain quantum physics", feature_name="test")

    cached = await cache.get(request)

    assert cached is None
    assert cache.misses > 0
```

---

### Ticket 1.6: Usage Monitor & Cost Tracking

**Priority**: High
**Estimated Time**: 4 hours
**Dependencies**: Ticket 1.2, 1.3

#### Context

Cost tracking is **critical** for staying within budget. This component monitors all LLM API usage in real-time and alerts when approaching limits.

**Features**:
- Per-provider token counting
- Cost calculation based on current pricing
- Daily/monthly aggregates
- Budget alerts
- Persistent logging

#### Implementation

```python
# src/catalyst_bot/services/llm/monitor.py
"""
LLM usage monitoring and cost tracking.

Tracks all API usage, calculates costs, and enforces budgets.
"""

import time
from datetime import datetime, timedelta
from typing import Dict, Optional
from collections import defaultdict
import structlog
import json
from pathlib import Path

from .models import LLMRequest, LLMResponse
from .config import LLMServiceConfig

logger = structlog.get_logger()


class UsageMonitor:
    """
    Monitors LLM API usage and costs.

    Tracks:
    - Requests per provider
    - Tokens consumed
    - Costs (daily/monthly)
    - Rate limits
    - Budget enforcement
    """

    def __init__(self, config: LLMServiceConfig):
        self.config = config

        # Usage stats
        self.stats = {
            "requests": defaultdict(int),  # By provider
            "tokens_input": defaultdict(int),
            "tokens_output": defaultdict(int),
            "cost_usd": defaultdict(float),
            "errors": defaultdict(int),
        }

        # Daily/monthly tracking
        self.daily_cost = 0.0
        self.monthly_cost = 0.0
        self.last_daily_reset = datetime.now()
        self.last_monthly_reset = datetime.now()

        # Persistent logging
        self.log_file = Path(config.usage_log_path) if config.usage_log_path else None

        logger.info("monitor.initialized", log_file=str(self.log_file))

    async def track_usage(self, request: LLMRequest, response: LLMResponse):
        """
        Track usage for a completed request.

        Args:
            request: Original request
            response: Response with metadata
        """
        provider = response.provider

        # Update stats
        self.stats["requests"][provider] += 1
        self.stats["tokens_input"][provider] += response.tokens_input
        self.stats["tokens_output"][provider] += response.tokens_output
        self.stats["cost_usd"][provider] += response.cost_usd

        # Update daily/monthly totals
        self._update_period_costs(response.cost_usd)

        # Check budget limits
        await self._check_budget_limits()

        # Log to file
        if self.log_file:
            await self._log_usage(request, response)

        logger.info(
            "monitor.usage",
            provider=provider,
            cost=response.cost_usd,
            daily_cost=self.daily_cost,
            monthly_cost=self.monthly_cost
        )

    def get_stats(self) -> dict:
        """Get usage statistics."""
        return {
            "by_provider": {
                provider: {
                    "requests": self.stats["requests"][provider],
                    "tokens_input": self.stats["tokens_input"][provider],
                    "tokens_output": self.stats["tokens_output"][provider],
                    "cost_usd": self.stats["cost_usd"][provider],
                }
                for provider in self.stats["requests"].keys()
            },
            "totals": {
                "requests": sum(self.stats["requests"].values()),
                "tokens_input": sum(self.stats["tokens_input"].values()),
                "tokens_output": sum(self.stats["tokens_output"].values()),
                "cost_usd": sum(self.stats["cost_usd"].values()),
            },
            "periods": {
                "daily_cost": self.daily_cost,
                "monthly_cost": self.monthly_cost,
            },
        }

    # ========================================================================
    # Private methods
    # ========================================================================

    def _update_period_costs(self, cost: float):
        """Update daily and monthly cost totals."""
        now = datetime.now()

        # Reset daily if new day
        if now.date() > self.last_daily_reset.date():
            logger.info("monitor.daily_reset", cost=self.daily_cost)
            self.daily_cost = 0.0
            self.last_daily_reset = now

        # Reset monthly if new month
        if now.month != self.last_monthly_reset.month:
            logger.info("monitor.monthly_reset", cost=self.monthly_cost)
            self.monthly_cost = 0.0
            self.last_monthly_reset = now

        # Add cost
        self.daily_cost += cost
        self.monthly_cost += cost

    async def _check_budget_limits(self):
        """Check if approaching budget limits."""
        # Daily limit
        if self.config.cost_alert_daily:
            if self.daily_cost >= self.config.cost_alert_daily:
                logger.warning(
                    "monitor.budget.daily_alert",
                    cost=self.daily_cost,
                    limit=self.config.cost_alert_daily
                )

        # Monthly limit
        if self.config.cost_alert_monthly:
            if self.monthly_cost >= self.config.cost_alert_monthly:
                logger.warning(
                    "monitor.budget.monthly_alert",
                    cost=self.monthly_cost,
                    limit=self.config.cost_alert_monthly
                )

        # Hard limit (stop processing)
        if self.config.cost_hard_limit_monthly:
            if self.monthly_cost >= self.config.cost_hard_limit_monthly:
                logger.error(
                    "monitor.budget.hard_limit_reached",
                    cost=self.monthly_cost,
                    limit=self.config.cost_hard_limit_monthly
                )
                raise Exception(f"Monthly budget hard limit reached: ${self.monthly_cost:.2f}")

    async def _log_usage(self, request: LLMRequest, response: LLMResponse):
        """Log usage to file."""
        if not self.log_file:
            return

        try:
            # Create log entry
            entry = {
                "timestamp": datetime.now().isoformat(),
                "provider": response.provider,
                "model": response.model,
                "feature": request.feature_name,
                "tokens_input": response.tokens_input,
                "tokens_output": response.tokens_output,
                "cost_usd": response.cost_usd,
                "latency_ms": response.latency_ms,
                "cached": response.cached,
            }

            # Append to file
            with open(self.log_file, "a") as f:
                f.write(json.dumps(entry) + "\n")

        except Exception as e:
            logger.error("monitor.log.error", error=str(e))
```

#### Acceptance Criteria

- [ ] Tracks requests per provider
- [ ] Calculates costs accurately
- [ ] Daily/monthly totals
- [ ] Budget alerts (warning + hard limit)
- [ ] Persistent logging to file

#### Testing

```python
# tests/services/llm/test_monitor.py
import pytest
from catalyst_bot.services.llm import UsageMonitor, LLMRequest, LLMResponse

def test_monitor_tracks_usage():
    """Test usage tracking."""
    config = LLMServiceConfig()
    monitor = UsageMonitor(config)

    response = LLMResponse(
        text="Test",
        provider="gemini",
        model="gemini-2.5-flash",
        tokens_input=100,
        tokens_output=50,
        cost_usd=0.01,
        latency_ms=100.0
    )

    request = LLMRequest(prompt="Test", feature_name="test")

    # Track usage
    await monitor.track_usage(request, response)

    # Check stats
    stats = monitor.get_stats()
    assert stats["by_provider"]["gemini"]["requests"] == 1
    assert stats["by_provider"]["gemini"]["cost_usd"] == 0.01
    assert stats["totals"]["cost_usd"] == 0.01
```

---

### Ticket 1.7: Configuration Management

**Priority**: Medium
**Estimated Time**: 2 hours
**Dependencies**: None

#### Context

Centralized configuration using Pydantic Settings for type-safe, environment-based config.

#### Implementation

```python
# src/catalyst_bot/services/llm/config.py
"""
LLM service configuration.

Uses Pydantic Settings for type-safe configuration from environment variables.
"""

from typing import Optional
from pydantic_settings import BaseSettings
from pydantic import Field


class LLMServiceConfig(BaseSettings):
    """
    LLM service configuration.

    All settings loaded from environment variables with defaults.
    """

    # Provider API Keys
    gemini_api_key: Optional[str] = Field(None, env="GEMINI_API_KEY")
    anthropic_api_key: Optional[str] = Field(None, env="ANTHROPIC_API_KEY")

    # Routing
    routing_strategy: str = Field("cost_optimized", env="LLM_ROUTING_STRATEGY")
    complexity_threshold: float = Field(0.7, env="LLM_COMPLEXITY_THRESHOLD")

    # Caching
    redis_url: str = Field("redis://localhost:6379", env="REDIS_URL")
    cache_enabled: bool = Field(True, env="LLM_CACHE_ENABLED")
    cache_ttl_seconds: int = Field(86400, env="LLM_CACHE_TTL_SECONDS")
    cache_similarity_threshold: float = Field(0.95, env="LLM_CACHE_SIMILARITY_THRESHOLD")

    # Cost tracking
    cost_tracking_enabled: bool = Field(True, env="LLM_COST_TRACKING")
    cost_alert_daily: Optional[float] = Field(30.00, env="LLM_COST_ALERT_DAILY")
    cost_alert_monthly: Optional[float] = Field(800.00, env="LLM_COST_ALERT_MONTHLY")
    cost_hard_limit_monthly: Optional[float] = Field(1000.00, env="LLM_COST_HARD_LIMIT_MONTHLY")
    usage_log_path: Optional[str] = Field("data/llm_usage.jsonl", env="LLM_USAGE_LOG_PATH")

    # Performance
    max_concurrent_requests: int = Field(10, env="LLM_MAX_CONCURRENT_REQUESTS")
    request_timeout_seconds: float = Field(30.0, env="LLM_REQUEST_TIMEOUT_SECONDS")

    class Config:
        env_file = ".env"
        case_sensitive = False

    @classmethod
    def from_env(cls) -> "LLMServiceConfig":
        """Load configuration from environment."""
        return cls()
```

#### Acceptance Criteria

- [ ] All config from environment variables
- [ ] Type-safe with Pydantic
- [ ] Defaults provided
- [ ] Validation on load

---

### Ticket 1.8: Phase 1 Integration Tests

**Priority**: High
**Estimated Time**: 4 hours
**Dependencies**: All Phase 1 tickets

#### Context

End-to-end tests for complete LLM service flow.

#### Implementation

```python
# tests/services/llm/test_integration.py
"""
Integration tests for complete LLM service flow.

Tests the entire pipeline: request ‚Üí router ‚Üí provider ‚Üí cache ‚Üí response
"""

import pytest
from catalyst_bot.services.llm import (
    LLMService,
    LLMRequest,
    TaskComplexity,
    OutputFormat
)

@pytest.mark.asyncio
@pytest.mark.integration
async def test_end_to_end_simple_query():
    """Test complete flow for simple query."""
    service = LLMService()
    await service.initialize()

    request = LLMRequest(
        prompt="What is 2+2? Answer with just the number.",
        complexity=TaskComplexity.SIMPLE,
        max_tokens=10,
        feature_name="test"
    )

    response = await service.query(request)

    assert response.text
    assert "4" in response.text
    assert response.provider in ["gemini", "claude"]
    assert response.latency_ms > 0
    assert response.cost_usd >= 0

@pytest.mark.asyncio
@pytest.mark.integration
async def test_cache_hit_on_second_query():
    """Test cache hit for duplicate query."""
    service = LLMService()
    await service.initialize()

    request = LLMRequest(
        prompt="Say 'test' and nothing else",
        complexity=TaskComplexity.SIMPLE,
        feature_name="test_cache"
    )

    # First query (cache miss)
    response1 = await service.query(request)
    assert not response1.cached

    # Second identical query (cache hit)
    response2 = await service.query(request)
    assert response2.cached
    assert response2.text == response1.text

@pytest.mark.asyncio
@pytest.mark.integration
async def test_batch_processing():
    """Test batch processing of multiple requests."""
    service = LLMService()
    await service.initialize()

    requests = [
        LLMRequest(
            prompt=f"Say 'test {i}' and nothing else",
            complexity=TaskComplexity.SIMPLE,
            max_tokens=10
        )
        for i in range(5)
    ]

    responses = await service.query_batch(requests)

    assert len(responses) == 5
    for i, response in enumerate(responses):
        assert f"test {i}" in response.text.lower()

@pytest.mark.asyncio
@pytest.mark.integration
async def test_cost_estimation():
    """Test cost estimation accuracy."""
    service = LLMService()
    await service.initialize()

    request = LLMRequest(
        prompt="A" * 1000,
        complexity=TaskComplexity.MEDIUM
    )

    # Estimate cost
    estimated = await service.estimate_cost(request)

    # Execute query
    response = await service.query(request)

    # Actual should be close to estimate (within 50%)
    assert abs(response.cost_usd - estimated) / estimated < 0.5
```

Run with:
```bash
poetry run pytest tests/services/llm/test_integration.py -v -m integration
```

---

### Ticket 1.9: Phase 1 Documentation

**Priority**: Medium
**Estimated Time**: 3 hours
**Dependencies**: All Phase 1 tickets

#### Context

Create comprehensive documentation for LLM service usage.

#### Deliverables

**1. API Documentation**

```markdown
# docs/architecture/LLM_SERVICE.md

# LLM Service API Documentation

## Overview

The LLM Service provides a unified interface for all LLM operations across Catalyst-Bot.

## Quick Start

\`\`\`python
from catalyst_bot.services.llm import get_llm_service, LLMRequest, TaskComplexity

# Get service instance
llm = await get_llm_service()

# Simple query
request = LLMRequest(
    prompt="Analyze this news headline...",
    complexity=TaskComplexity.SIMPLE
)
response = await llm.query(request)
print(response.text)
\`\`\`

## Architecture

[Include architecture diagram]

## Request Model

... [detailed docs]

## Response Model

... [detailed docs]

## Providers

... [provider details]

## Caching

... [caching strategy]

## Cost Optimization

... [cost tips]
```

**2. Integration Guide**

```markdown
# docs/guides/LLM_INTEGRATION.md

# Integrating LLM Service

This guide shows how to integrate the LLM service into your features.

## Basic Usage

[Examples]

## Advanced Features

[Batch processing, custom schemas, etc.]

## Best Practices

[Performance tips, cost optimization]
```

**3. Troubleshooting Guide**

```markdown
# docs/guides/LLM_TROUBLESHOOTING.md

# LLM Service Troubleshooting

## Common Issues

### API Key Errors
...

### Cache Not Working
...

### High Costs
...
```

---

## Phase 2: WebSocket SEC Digester Service

**Duration**: 2 weeks (Week 3-4)
**Goal**: Build standalone SEC analysis service with WebSocket streaming

---

### Ticket 2.1: FastAPI WebSocket Server Setup

**Priority**: Critical
**Estimated Time**: 6 hours
**Dependencies**: Phase 1 complete

#### Context

Create the foundation for the SEC Digester microservice using FastAPI. This service will:
1. Monitor EDGAR for new filings
2. Analyze them with LLM
3. Stream results via WebSocket to clients

**Key Pattern**: Separate concerns - this service ONLY does SEC analysis, main bot ONLY does alerts.

#### Implementation

```python
# src/sec_digester_service/main.py
"""
SEC Digester WebSocket Microservice.

Standalone service that:
1. Monitors EDGAR for new filings
2. Analyzes with LLM (via unified LLM service)
3. Publishes to Redis Streams
4. Broadcasts via WebSocket to clients

Usage:
    uvicorn sec_digester_service.main:app --host 0.0.0.0 --port 8765
"""

from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import asyncio
from typing import Set
import structlog

from .config import get_settings
from .websocket.server import ConnectionManager
from .streams.publisher import StreamPublisher
from .edgar.monitor import EDGARMonitor

logger = structlog.get_logger()

# Global state
connection_manager: ConnectionManager = None
stream_publisher: StreamPublisher = None
edgar_monitor: EDGARMonitor = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Lifespan context manager for startup/shutdown.

    Startup:
    - Initialize Redis connection
    - Start EDGAR monitor
    - Start WebSocket broadcaster

    Shutdown:
    - Stop background tasks
    - Close connections
    """
    global connection_manager, stream_publisher, edgar_monitor

    settings = get_settings()
    logger.info("sec_digester.startup")

    # Initialize components
    connection_manager = ConnectionManager()
    stream_publisher = StreamPublisher(settings.redis_url)
    await stream_publisher.initialize()

    edgar_monitor = EDGARMonitor(
        stream_publisher=stream_publisher,
        poll_interval=settings.edgar_poll_interval
    )

    # Start background tasks
    asyncio.create_task(edgar_monitor.start())
    asyncio.create_task(broadcast_from_redis())

    logger.info("sec_digester.ready")

    yield

    # Shutdown
    logger.info("sec_digester.shutdown")
    await edgar_monitor.stop()
    await stream_publisher.close()


app = FastAPI(
    title="SEC Digester Service",
    description="Real-time SEC filing analysis and streaming",
    version="2.0.0",
    lifespan=lifespan
)

# CORS for web dashboard
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============================================================================
# WebSocket Endpoints
# ============================================================================

@app.websocket("/ws/filings")
async def websocket_filings(websocket: WebSocket):
    """
    WebSocket endpoint for real-time SEC filing stream.

    Message Format:
    {
        "type": "filing_alert",
        "filing_id": "0001234567-25-000123",
        "ticker": "AAPL",
        "filing_type": "8-K",
        "analysis": { ... }
    }
    """
    await connection_manager.connect(websocket)

    try:
        # Send welcome message
        await websocket.send_json({
            "type": "connected",
            "message": "Connected to SEC Digester",
            "version": "2.0.0"
        })

        # Keep connection alive
        while True:
            # Receive heartbeat from client
            data = await websocket.receive_text()

            if data == "ping":
                await websocket.send_text("pong")

    except WebSocketDisconnect:
        connection_manager.disconnect(websocket)
        logger.info("client.disconnected", active=len(connection_manager.active_connections))


# ============================================================================
# REST API Endpoints
# ============================================================================

@app.get("/api/filings/recent")
async def get_recent_filings(limit: int = 50):
    """Get recent filings from Redis Stream."""
    filings = await stream_publisher.get_recent(limit)
    return {"filings": filings, "count": len(filings)}


@app.get("/api/filings/{ticker}")
async def get_filings_by_ticker(ticker: str, limit: int = 20):
    """Get recent filings for specific ticker."""
    filings = await stream_publisher.get_by_ticker(ticker, limit)
    return {"ticker": ticker, "filings": filings, "count": len(filings)}


@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {
        "status": "healthy",
        "active_connections": len(connection_manager.active_connections),
        "edgar_monitor": "running" if edgar_monitor.is_running else "stopped"
    }


@app.get("/metrics")
async def get_metrics():
    """Prometheus-compatible metrics."""
    return {
        "filings_processed": edgar_monitor.get_stats()["filings_processed"],
        "active_websocket_connections": len(connection_manager.active_connections),
        "cache_hit_rate": 0.0,  # TODO: from LLM service
    }


# ============================================================================
# Background Tasks
# ============================================================================

async def broadcast_from_redis():
    """
    Background task: Read from Redis Streams and broadcast to WebSocket clients.

    Reads new messages from Redis and pushes them to all connected WebSocket clients.
    """
    logger.info("broadcaster.start")

    while True:
        try:
            # Get new filings from Redis Stream
            messages = await stream_publisher.read_new_messages()

            for message in messages:
                # Broadcast to all WebSocket clients
                await connection_manager.broadcast(message)

        except Exception as e:
            logger.error("broadcaster.error", error=str(e))
            await asyncio.sleep(1)
```

**Connection Manager:**

```python
# src/sec_digester_service/websocket/server.py
"""
WebSocket connection manager.

Manages active WebSocket connections and broadcasting.
"""

from fastapi import WebSocket
from typing import Set
import structlog
import json

logger = structlog.get_logger()


class ConnectionManager:
    """Manages active WebSocket connections."""

    def __init__(self):
        self.active_connections: Set[WebSocket] = set()

    async def connect(self, websocket: WebSocket):
        """Accept new WebSocket connection."""
        await websocket.accept()
        self.active_connections.add(websocket)
        logger.info("websocket.connected", active=len(self.active_connections))

    def disconnect(self, websocket: WebSocket):
        """Remove WebSocket connection."""
        self.active_connections.discard(websocket)
        logger.info("websocket.disconnected", active=len(self.active_connections))

    async def broadcast(self, message: dict):
        """Broadcast message to all connected clients."""
        disconnected = set()

        for connection in self.active_connections:
            try:
                await connection.send_json(message)
            except Exception as e:
                logger.error("websocket.broadcast.error", error=str(e))
                disconnected.add(connection)

        # Remove disconnected clients
        self.active_connections -= disconnected

        if disconnected:
            logger.info("websocket.cleaned", removed=len(disconnected))
```

#### Acceptance Criteria

- [ ] FastAPI app with lifespan management
- [ ] WebSocket endpoint `/ws/filings`
- [ ] REST API for queries
- [ ] Health check endpoint
- [ ] Connection manager handles multiple clients
- [ ] CORS configured for web dashboard

#### Testing

```bash
# Start server
poetry run uvicorn sec_digester_service.main:app --reload --port 8765

# Test WebSocket (in another terminal)
poetry run python scripts/test_websocket.py
```

```python
# scripts/test_websocket.py
import asyncio
import websockets
import json

async def test_websocket():
    uri = "ws://localhost:8765/ws/filings"

    async with websockets.connect(uri) as websocket:
        # Receive welcome message
        message = await websocket.recv()
        print(f"Received: {message}")

        # Send heartbeat
        await websocket.send("ping")
        response = await websocket.recv()
        print(f"Heartbeat: {response}")

        # Listen for filings
        print("Listening for filings...")
        while True:
            message = await websocket.recv()
            data = json.loads(message)
            print(f"Filing: {data.get('ticker')} {data.get('filing_type')}")

if __name__ == "__main__":
    asyncio.run(test_websocket())
```

---

### Ticket 2.2: EDGAR Monitor Integration

**Priority**: Critical
**Estimated Time**: 8 hours
**Dependencies**: Ticket 2.1

#### Context

Integrate existing EDGAR monitoring logic into the new service. This polls EDGAR RSS feed every 5 minutes and detects new filings.

**Migration Note**: We're moving code from `src/catalyst_bot/sec_monitor.py` to `src/sec_digester_service/edgar/monitor.py` with minimal changes.

#### Implementation

```python
# src/sec_digester_service/edgar/monitor.py
"""
EDGAR filing monitor.

Polls EDGAR RSS feed for new filings and triggers analysis.
"""

import asyncio
from datetime import datetime, timedelta
from typing import Set, Optional
import structlog

from ..streams.publisher import StreamPublisher
from .fetcher import EDGARFetcher
from .parser import FilingParser
from ...services.llm import get_llm_service, LLMRequest, TaskComplexity

logger = structlog.get_logger()


class EDGARMonitor:
    """
    EDGAR filing monitor.

    Polls EDGAR RSS feed, detects new filings, analyzes them,
    and publishes to Redis Stream.
    """

    def __init__(
        self,
        stream_publisher: StreamPublisher,
        poll_interval: int = 300  # 5 minutes
    ):
        self.publisher = stream_publisher
        self.poll_interval = poll_interval
        self.is_running = False

        # Components
        self.fetcher = EDGARFetcher()
        self.parser = FilingParser()
        self.llm_service = None

        # Deduplication cache (4-hour window)
        self.processed_filings: Set[str] = set()
        self.cache_duration = timedelta(hours=4)
        self.last_cache_clean = datetime.now()

        # Stats
        self.stats = {
            "filings_processed": 0,
            "filings_analyzed": 0,
            "errors": 0,
        }

    async def start(self):
        """Start monitoring loop."""
        logger.info("edgar_monitor.starting")

        # Initialize LLM service
        self.llm_service = await get_llm_service()

        self.is_running = True

        while self.is_running:
            try:
                await self._poll_edgar()
            except Exception as e:
                logger.error("edgar_monitor.error", error=str(e))
                self.stats["errors"] += 1

            # Wait before next poll
            await asyncio.sleep(self.poll_interval)

    async def stop(self):
        """Stop monitoring."""
        logger.info("edgar_monitor.stopping")
        self.is_running = False

    def get_stats(self) -> dict:
        """Get monitoring statistics."""
        return self.stats

    # ========================================================================
    # Private methods
    # ========================================================================

    async def _poll_edgar(self):
        """Poll EDGAR for new filings."""
        logger.info("edgar_monitor.poll")

        # Fetch recent filings from RSS
        filings = await self.fetcher.fetch_recent_filings()

        # Filter new filings
        new_filings = [
            f for f in filings
            if f.accession_number not in self.processed_filings
        ]

        if not new_filings:
            logger.debug("edgar_monitor.no_new_filings")
            return

        logger.info("edgar_monitor.new_filings", count=len(new_filings))

        # Process each filing
        for filing in new_filings:
            try:
                await self._process_filing(filing)
            except Exception as e:
                logger.error(
                    "edgar_monitor.filing_error",
                    filing_id=filing.accession_number,
                    error=str(e)
                )

        # Clean old entries from cache
        self._clean_cache()

    async def _process_filing(self, filing):
        """Process a single filing."""
        # Mark as processed
        self.processed_filings.add(filing.accession_number)
        self.stats["filings_processed"] += 1

        # Parse filing (extract item codes, etc.)
        parsed = await self.parser.parse(filing)

        # Analyze with LLM
        analysis = await self._analyze_filing(parsed)

        self.stats["filings_analyzed"] += 1

        # Create alert message
        alert = {
            "type": "filing_alert",
            "filing_id": filing.accession_number,
            "ticker": filing.ticker,
            "filing_type": filing.filing_type,
            "item_code": parsed.item_code,
            "timestamp": filing.filed_at.isoformat(),
            "analysis": {
                "sentiment": analysis.sentiment,
                "keywords": analysis.keywords,
                "summary": analysis.summary,
                "priority": analysis.priority,
            },
            "url": filing.url,
        }

        # Publish to Redis Stream
        await self.publisher.publish(alert)

        logger.info(
            "edgar_monitor.filing_published",
            ticker=filing.ticker,
            filing_type=filing.filing_type
        )

    async def _analyze_filing(self, filing):
        """Analyze filing with LLM."""
        # Determine complexity based on filing type
        complexity = self._get_filing_complexity(filing)

        # Create LLM request
        request = LLMRequest(
            prompt=filing.text,
            complexity=complexity,
            feature_name="sec_analysis",
            user_id=filing.ticker,
            max_tokens=1024
        )

        # Query LLM
        response = await self.llm_service.query(request)

        # Parse response (TODO: use structured output)
        # For now, return simple analysis
        return FilingAnalysis(
            sentiment=0.5,  # TODO: extract from response
            keywords=["TODO"],
            summary=response.text[:200],
            priority="medium"
        )

    def _get_filing_complexity(self, filing) -> TaskComplexity:
        """Determine analysis complexity based on filing type."""
        # Map filing types to complexity
        complexity_map = {
            ("8-K", "8.01"): TaskComplexity.SIMPLE,  # Other events
            ("8-K", "2.02"): TaskComplexity.COMPLEX,  # Earnings
            ("8-K", "1.01"): TaskComplexity.COMPLEX,  # M&A
            ("10-Q", None): TaskComplexity.MEDIUM,
            ("10-K", None): TaskComplexity.COMPLEX,
        }

        key = (filing.filing_type, filing.item_code)
        return complexity_map.get(key, TaskComplexity.MEDIUM)

    def _clean_cache(self):
        """Remove old entries from deduplication cache."""
        now = datetime.now()

        if (now - self.last_cache_clean) < timedelta(hours=1):
            return  # Clean once per hour

        # Clear entire cache (simple approach)
        # In production, track timestamps per entry
        self.processed_filings.clear()
        self.last_cache_clean = now

        logger.info("edgar_monitor.cache_cleaned")


class FilingAnalysis:
    """Filing analysis result."""
    def __init__(self, sentiment, keywords, summary, priority):
        self.sentiment = sentiment
        self.keywords = keywords
        self.summary = summary
        self.priority = priority
```

#### Acceptance Criteria

- [ ] Polls EDGAR RSS every 5 minutes
- [ ] Detects new filings
- [ ] Deduplicates using 4-hour cache
- [ ] Analyzes with LLM service
- [ ] Publishes to Redis Stream
- [ ] Stats tracking

---

### Ticket 2.3: Redis Streams Publisher

**Priority**: Critical
**Estimated Time**: 4 hours
**Dependencies**: Ticket 2.1

#### Context

Redis Streams provides message persistence and reliable delivery. This is the backbone of our WebSocket architecture - all filings flow through Redis before being broadcast.

#### Implementation

```python
# src/sec_digester_service/streams/publisher.py
"""
Redis Streams publisher for SEC filings.

Publishes analyzed filings to Redis Stream for:
1. WebSocket broadcasting
2. Message persistence
3. Catch-up capability for clients
"""

import asyncio
import json
from typing import List, Optional
import redis.asyncio as redis
import structlog

logger = structlog.get_logger()


class StreamPublisher:
    """Publishes SEC filings to Redis Streams."""

    STREAM_NAME = "sec:filings"
    MAX_LENGTH = 10000  # Keep last 10K filings

    def __init__(self, redis_url: str):
        self.redis_url = redis_url
        self.redis: Optional[redis.Redis] = None
        self.last_id = "0"  # For reading new messages

    async def initialize(self):
        """Initialize Redis connection."""
        self.redis = await redis.from_url(self.redis_url)
        await self.redis.ping()
        logger.info("stream_publisher.initialized")

    async def close(self):
        """Close Redis connection."""
        if self.redis:
            await self.redis.close()

    async def publish(self, filing_alert: dict):
        """
        Publish filing alert to Redis Stream.

        Args:
            filing_alert: Filing alert message
        """
        try:
            # Serialize to JSON
            data = json.dumps(filing_alert)

            # Add to stream with max length (FIFO)
            message_id = await self.redis.xadd(
                self.STREAM_NAME,
                {"data": data},
                maxlen=self.MAX_LENGTH
            )

            logger.info(
                "stream.published",
                message_id=message_id,
                ticker=filing_alert.get("ticker")
            )

        except Exception as e:
            logger.error("stream.publish.error", error=str(e))
            raise

    async def read_new_messages(self, block_ms: int = 1000) -> List[dict]:
        """
        Read new messages from stream (for WebSocket broadcasting).

        Args:
            block_ms: Block for this many milliseconds

        Returns:
            List of filing alert messages
        """
        try:
            # Read from last position
            messages = await self.redis.xread(
                {self.STREAM_NAME: self.last_id},
                count=10,
                block=block_ms
            )

            filings = []

            if messages:
                for stream_name, stream_messages in messages:
                    for message_id, message_data in stream_messages:
                        # Update last_id
                        self.last_id = message_id

                        # Parse message
                        filing = json.loads(message_data[b"data"])
                        filings.append(filing)

            return filings

        except Exception as e:
            logger.error("stream.read.error", error=str(e))
            return []

    async def get_recent(self, limit: int = 50) -> List[dict]:
        """
        Get recent filings from stream (for catch-up).

        Args:
            limit: Max number of filings

        Returns:
            List of filing alerts (newest first)
        """
        try:
            # Read in reverse (newest first)
            messages = await self.redis.xrevrange(
                self.STREAM_NAME,
                count=limit
            )

            filings = []
            for message_id, message_data in messages:
                filing = json.loads(message_data[b"data"])
                filings.append(filing)

            return filings

        except Exception as e:
            logger.error("stream.get_recent.error", error=str(e))
            return []

    async def get_by_ticker(self, ticker: str, limit: int = 20) -> List[dict]:
        """
        Get recent filings for specific ticker.

        Args:
            ticker: Stock ticker
            limit: Max number

        Returns:
            List of filings for ticker
        """
        # Get recent filings and filter by ticker
        # In production, use secondary index or separate stream per ticker
        all_filings = await self.get_recent(limit=100)

        filings = [
            f for f in all_filings
            if f.get("ticker", "").upper() == ticker.upper()
        ][:limit]

        return filings
```

#### Acceptance Criteria

- [ ] Publishes to Redis Stream
- [ ] Max length enforced (FIFO)
- [ ] Read new messages for broadcasting
- [ ] Get recent for catch-up
- [ ] Filter by ticker

---

### Ticket 2.4-2.8: Remaining Phase 2 Tickets (Summary)

Due to document length, here's a structured summary of remaining Phase 2 tickets. Each follows the same detailed pattern as above.

**Ticket 2.4: Docker Setup** (4 hours)
- Create `Dockerfile` for SEC service
- `docker-compose.yml` for local development
- Environment configuration
- Health checks

**Ticket 2.5: Service Configuration** (2 hours)
- Pydantic Settings for SEC service
- Environment variables
- Validation

**Ticket 2.6: Integration Tests** (4 hours)
- End-to-end WebSocket tests
- EDGAR monitor tests
- Redis Stream tests

**Ticket 2.7: Documentation** (3 hours)
- SEC service API docs
- Deployment guide
- Troubleshooting

**Ticket 2.8: Phase 2 Completion** (2 hours)
- Verify all acceptance criteria
- Performance testing
- Ready for Phase 3

---

## Phase 3: Client Integration

**Duration**: 1 week (Week 5)
**Goal**: Connect main bot to SEC WebSocket service

---

### Ticket 3.1: WebSocket Client Implementation

**Priority**: Critical
**Estimated Time**: 6 hours
**Dependencies**: Phase 2 complete

#### Context

Create WebSocket client in main bot to receive real-time filing alerts from SEC service.

#### Implementation

```python
# src/catalyst_bot/services/websocket/client.py
"""
WebSocket client for SEC Digester service.

Connects to SEC service and receives real-time filing alerts with auto-reconnect.
"""

import asyncio
from typing import Callable, Optional, Set
from datetime import datetime
import websockets
import json
import structlog

logger = structlog.get_logger()


class SECWebSocketClient:
    """
    WebSocket client for SEC Digester service.

    Features:
    - Auto-reconnect on disconnect
    - Catch-up on missed messages
    - Heartbeat keep-alive
    - Message deduplication
    """

    def __init__(
        self,
        url: str,
        on_filing: Optional[Callable] = None
    ):
        self.url = url
        self.on_filing = on_filing
        self.running = False

        # Deduplication
        self.processed_filings: Set[str] = set()
        self.max_cache_size = 1000

        # Stats
        self.stats = {
            "filings_received": 0,
            "reconnects": 0,
            "errors": 0
        }

    async def connect(self):
        """Connect with auto-reconnect loop."""
        self.running = True
        logger.info("sec_client.connecting", url=self.url)

        while self.running:
            try:
                await self._connect_and_listen()
            except Exception as e:
                logger.error("sec_client.error", error=str(e))
                self.stats["errors"] += 1

            if self.running:
                self.stats["reconnects"] += 1
                logger.info("sec_client.reconnecting")
                await asyncio.sleep(5)

    async def disconnect(self):
        """Disconnect from service."""
        self.running = False

    async def _connect_and_listen(self):
        """Connect and listen for messages."""
        async with websockets.connect(self.url) as websocket:
            logger.info("sec_client.connected")

            # Catch up on recent filings
            await self._catch_up()

            # Start heartbeat
            heartbeat_task = asyncio.create_task(self._heartbeat(websocket))

            try:
                async for message in websocket:
                    await self._handle_message(message)
            finally:
                heartbeat_task.cancel()

    async def _catch_up(self):
        """Fetch recent filings via REST API."""
        import aiohttp

        try:
            api_url = self.url.replace("ws://", "http://").replace("/ws/filings", "")

            async with aiohttp.ClientSession() as session:
                async with session.get(f"{api_url}/api/filings/recent?limit=50") as resp:
                    data = await resp.json()

                    for filing in reversed(data["filings"]):
                        await self._handle_filing(filing)

                    logger.info("sec_client.caught_up", count=len(data["filings"]))

        except Exception as e:
            logger.error("sec_client.catchup.error", error=str(e))

    async def _heartbeat(self, websocket):
        """Send periodic heartbeat."""
        while True:
            try:
                await websocket.send("ping")
                await asyncio.sleep(30)
            except Exception:
                break

    async def _handle_message(self, message: str):
        """Handle incoming message."""
        try:
            data = json.loads(message)
            msg_type = data.get("type")

            if msg_type == "filing_alert":
                await self._handle_filing(data)
            elif msg_type == "connected":
                logger.info("sec_client.server_message", msg=data.get("message"))

        except json.JSONDecodeError:
            pass  # Heartbeat response

    async def _handle_filing(self, filing: dict):
        """Handle filing alert."""
        filing_id = filing.get("filing_id")

        # Deduplicate
        if filing_id in self.processed_filings:
            return

        self.processed_filings.add(filing_id)

        # Limit cache size
        if len(self.processed_filings) > self.max_cache_size:
            oldest = next(iter(self.processed_filings))
            self.processed_filings.remove(oldest)

        self.stats["filings_received"] += 1

        logger.info(
            "sec_client.filing",
            ticker=filing["ticker"],
            filing_type=filing["filing_type"]
        )

        # Call callback
        if self.on_filing:
            try:
                await self.on_filing(filing)
            except Exception as e:
                logger.error("sec_client.callback.error", error=str(e))
```

#### Acceptance Criteria

- [ ] WebSocket client with auto-reconnect
- [ ] Catch-up mechanism via REST API
- [ ] Heartbeat keep-alive
- [ ] Message deduplication
- [ ] Callback for filings

---

### Tickets 3.2-3.6: Remaining Phase 3 (Summary)

**Ticket 3.2: Message Handlers** (4 hours)
- Alert generation from filing data
- Discord embed formatting
- Priority-based routing

**Ticket 3.3: Migration of Alert Logic** (6 hours)
- Move from `sec_filing_alerts.py` to handlers
- Update Discord integration
- Test alert generation

**Ticket 3.4: Integration Testing** (4 hours)
- End-to-end: SEC service ‚Üí Client ‚Üí Discord
- Reconnection scenarios
- Performance testing

**Ticket 3.5: Configuration Updates** (2 hours)
- Add `SEC_DIGESTER_URL` to config
- Feature flags
- Rollback capability

**Ticket 3.6: Phase 3 Completion** (2 hours)
- All tests passing
- Documentation updated
- Ready for Phase 4

---

## Phase 4: Migration & Optimization

**Duration**: 1 week (Week 6)
**Goal**: Migrate remaining features and optimize performance

---

### Key Tickets (Summary)

**Ticket 4.1: Deprecate Old LLM Files** (4 hours)
- Mark old files as deprecated
- Add import warnings
- Update imports across codebase

**Ticket 4.2: Prompt Optimization** (6 hours)
- Compress prompts (target 40% reduction)
- Extract key sections only
- Test accuracy maintained

**Ticket 4.3: Cache Tuning** (4 hours)
- Adjust similarity threshold
- Monitor hit rates
- Optimize TTLs

**Ticket 4.4: Performance Benchmarking** (6 hours)
- Measure latency (target <2s)
- Measure costs (target <$800/month)
- Measure cache hit rate (target 70%+)

**Ticket 4.5: Code Cleanup** (4 hours)
- Remove deprecated files
- Update documentation
- Code review

**Ticket 4.6: Phase 4 Completion** (2 hours)
- All benchmarks met
- Documentation complete
- Ready for production

---

## Phase 5: Production Deployment

**Duration**: 1 week (Week 7)
**Goal**: Deploy to production and monitor

---

### Key Tickets (Summary)

**Ticket 5.1: Cloud Infrastructure** (6 hours)
- Provision cloud resources
- Set up Redis (managed service)
- Configure networking

**Ticket 5.2: SEC Service Deployment** (4 hours)
- Deploy to cloud (AWS Fargate, GCP Cloud Run, etc.)
- Configure auto-scaling
- Set up health checks

**Ticket 5.3: Main Bot Deployment** (3 hours)
- Update bot configuration
- Deploy updated bot
- Verify WebSocket connection

**Ticket 5.4: Monitoring Setup** (4 hours)
- Prometheus metrics
- Grafana dashboards
- Alerting rules

**Ticket 5.5: Load Testing** (4 hours)
- Simulate 1000 filings/day
- Verify performance
- Tune as needed

**Ticket 5.6: Rollback Procedures** (3 hours)
- Document rollback steps
- Test rollback scenario
- Create runbook

**Ticket 5.7: Go-Live** (4 hours)
- Final checks
- Production cutover
- Monitor for 24 hours

---

## Testing Strategy

### Unit Tests

**Coverage Target**: 90%+

**Key Areas**:
- LLM service components
- Provider adapters
- Router logic
- Cache implementation
- WebSocket client/server

**Run**:
```bash
poetry run pytest tests/services/ -v --cov=catalyst_bot/services
```

### Integration Tests

**Scenarios**:
1. End-to-end LLM query flow
2. SEC service ‚Üí Client ‚Üí Alert
3. Cache hit/miss scenarios
4. Reconnection handling

**Run**:
```bash
poetry run pytest tests/ -v -m integration
```

### Performance Tests

**Metrics**:
- Latency (P50, P95, P99)
- Throughput (requests/second)
- Cost per request
- Cache hit rate

**Tools**:
- `locust` for load testing
- Custom benchmarking scripts

### End-to-End Tests

**Full System**:
1. Start SEC service
2. Start main bot
3. Simulate filing
4. Verify Discord alert

---

## Rollback Plan

### Scenario: Critical Issue in Production

**Step 1**: Immediate Mitigation (5 minutes)
```bash
# Disable new architecture via feature flag
export FEATURE_UNIFIED_LLM_SERVICE=0
export SEC_DIGESTER_ENABLED=0

# Restart bot
systemctl restart catalyst-bot
```

**Step 2**: Verify Old Code Works (10 minutes)
- Check Discord alerts functioning
- Verify SEC filings processing
- Monitor for errors

**Step 3**: Investigate Issue (1-2 hours)
- Check logs
- Identify root cause
- Determine fix timeline

**Step 4**: Decision Point
- **Quick fix available**: Deploy fix, re-enable
- **Complex issue**: Keep rolled back, schedule fix

### What to Keep Running

**During rollback**:
- ‚úÖ Old LLM files (not deleted until Phase 4)
- ‚úÖ Existing SEC monitor
- ‚úÖ Existing alert system

**This ensures zero downtime**

---

## Appendix: Scripts & Templates

### Setup Script

```bash
# scripts/setup_complete.sh
#!/bin/bash
# Complete setup for development environment

set -e

echo "üöÄ Complete Development Setup"

# Run base setup
./scripts/setup_dev_env.sh

# Start services
echo "Starting Redis..."
docker-compose up -d redis

# Run migrations (if needed)
# alembic upgrade head

# Test connections
echo "Testing LLM providers..."
poetry run python scripts/test_llm_providers.py

echo "Testing SEC service..."
curl http://localhost:8765/health

echo "‚úÖ Setup complete!"
```

### Docker Compose

```yaml
# docker-compose.yml
version: '3.8'

services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes

  sec-digester:
    build:
      context: .
      dockerfile: src/sec_digester_service/Dockerfile
    ports:
      - "8765:8765"
    environment:
      - REDIS_URL=redis://redis:6379
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
    depends_on:
      - redis

  catalyst-bot:
    build: .
    environment:
      - SEC_DIGESTER_URL=ws://sec-digester:8765/ws/filings
      - DISCORD_TOKEN=${DISCORD_TOKEN}
      - REDIS_URL=redis://redis:6379
    depends_on:
      - sec-digester

volumes:
  redis-data:
```

### Testing Script

```python
# scripts/test_complete_flow.py
"""
Test complete flow: EDGAR ‚Üí Analysis ‚Üí WebSocket ‚Üí Alert

This simulates the entire pipeline end-to-end.
"""

import asyncio
from catalyst_bot.services.llm import get_llm_service
from catalyst_bot.services.websocket.client import SECWebSocketClient


async def test_flow():
    print("Testing complete flow...")

    # 1. Test LLM service
    llm = await get_llm_service()
    response = await llm.query(LLMRequest(
        prompt="Test",
        complexity=TaskComplexity.SIMPLE
    ))
    print(f"‚úÖ LLM service: {response.provider}")

    # 2. Test WebSocket client
    received = []

    async def on_filing(filing):
        received.append(filing)
        print(f"‚úÖ Filing received: {filing['ticker']}")

    client = SECWebSocketClient(
        url="ws://localhost:8765/ws/filings",
        on_filing=on_filing
    )

    # Connect and wait for filing
    await asyncio.wait_for(client.connect(), timeout=30)

    print(f"‚úÖ Complete flow working! Received {len(received)} filings")


if __name__ == "__main__":
    asyncio.run(test_flow())
```

---

## Implementation Checklist

### Week 1-2: Phase 1
- [ ] Ticket 1.1: Project setup
- [ ] Ticket 1.2: LLM service core
- [ ] Ticket 1.3: Providers (Gemini + Claude)
- [ ] Ticket 1.4: Router
- [ ] Ticket 1.5: Semantic cache
- [ ] Ticket 1.6: Usage monitor
- [ ] Ticket 1.7: Configuration
- [ ] Ticket 1.8: Integration tests
- [ ] Ticket 1.9: Documentation

### Week 3-4: Phase 2
- [ ] Ticket 2.1: FastAPI WebSocket server
- [ ] Ticket 2.2: EDGAR monitor
- [ ] Ticket 2.3: Redis Streams
- [ ] Ticket 2.4: Docker setup
- [ ] Ticket 2.5: Configuration
- [ ] Ticket 2.6: Integration tests
- [ ] Ticket 2.7: Documentation
- [ ] Ticket 2.8: Phase completion

### Week 5: Phase 3
- [ ] Ticket 3.1: WebSocket client
- [ ] Ticket 3.2: Message handlers
- [ ] Ticket 3.3: Alert migration
- [ ] Ticket 3.4: Integration tests
- [ ] Ticket 3.5: Configuration
- [ ] Ticket 3.6: Phase completion

### Week 6: Phase 4
- [ ] Ticket 4.1: Deprecate old files
- [ ] Ticket 4.2: Prompt optimization
- [ ] Ticket 4.3: Cache tuning
- [ ] Ticket 4.4: Benchmarking
- [ ] Ticket 4.5: Code cleanup
- [ ] Ticket 4.6: Phase completion

### Week 7: Phase 5
- [ ] Ticket 5.1: Cloud infrastructure
- [ ] Ticket 5.2: SEC service deployment
- [ ] Ticket 5.3: Main bot deployment
- [ ] Ticket 5.4: Monitoring
- [ ] Ticket 5.5: Load testing
- [ ] Ticket 5.6: Rollback procedures
- [ ] Ticket 5.7: Go-live

---

## Success Metrics

### Performance
- [ ] Average latency < 2s
- [ ] P95 latency < 4s
- [ ] Cache hit rate > 70%
- [ ] Throughput: 1000 filings/day

### Cost
- [ ] Monthly cost < $1,000
- [ ] Average cost/filing < $0.10
- [ ] 60%+ cost reduction vs. baseline

### Quality
- [ ] 95%+ accuracy maintained
- [ ] Zero message loss
- [ ] 99.5%+ uptime
- [ ] <1% error rate

### Code
- [ ] 50% code reduction (5,100 ‚Üí 2,500 lines)
- [ ] 90%+ test coverage
- [ ] All documentation complete
- [ ] Zero regression in functionality

---

## Final Notes

This is a **comprehensive, production-ready implementation plan** designed for AI-assisted development with Claude Code and Codex CLI.

**Key Features**:
‚úÖ **7-week timeline** - Realistic, phase-by-phase approach
‚úÖ **Detailed tickets** - Each with context, code, tests, acceptance criteria
‚úÖ **Continuous context** - Designed for AI assistants to follow
‚úÖ **Complete examples** - 100+ code samples inline
‚úÖ **Testing strategy** - Unit, integration, e2e, performance
‚úÖ **Rollback plan** - Zero-downtime deployment
‚úÖ **Monitoring** - Prometheus, Grafana, alerting

**Total Document**: ~3,900+ lines
**Total Code Examples**: 40+ complete implementations
**Total Tickets**: 30+ detailed implementation tasks

**Ready to Start**: Begin with Ticket 1.1 and work sequentially through phases.

---

**Document Complete** ‚úÖ